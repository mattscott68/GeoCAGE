{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F7n67NXnETx"
      },
      "source": [
        "### REQUIRE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbauekVAnIXa",
        "outputId": "4017a43c-8c10-4906-e00c-a102563411ad"
      },
      "outputs": [],
      "source": [
        "!pip install torchviz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZvfKlejSPFu"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "OgNlQ0UQSRo1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "class Util_class():\n",
        "\n",
        "  \"\"\"\n",
        "  dict_in : dictionary\n",
        "  list_key : list of key\n",
        "  return True if ALL key in list_key is in dict\n",
        "  \"\"\"\n",
        "  @staticmethod\n",
        "  def check_key_in_dict(dict_in, list_key):\n",
        "    is_in = True\n",
        "    list_not_in = []\n",
        "    for key in list_key:\n",
        "      if key not in dict_in:\n",
        "        is_in = False\n",
        "        list_not_in.append(key)\n",
        "    return [is_in,list_not_in]\n",
        "\n",
        "  @staticmethod\n",
        "  def same_key_in_dict(dict_in, list_key):\n",
        "    is_in = True\n",
        "    key_not_dict = []\n",
        "    key_not_list = []\n",
        "\n",
        "    for key in list_key:\n",
        "      if key not in dict_in:\n",
        "        is_in = False\n",
        "        key_not_dict.append(key)\n",
        "\n",
        "    for key in dict_in:\n",
        "      if key not in list_key:\n",
        "        is_in = False\n",
        "        key_not_list.append(key)\n",
        "    return [is_in,key_not_dict,key_not_list]\n",
        "\n",
        "  @staticmethod\n",
        "  def folder_manage(path, uniquify=True,clean=False, force=False):\n",
        "    last_folder = os.path.basename(os.path.normpath(path))\n",
        "    head_path = os.path.dirname(os.path.normpath(path))\n",
        "\n",
        "    #head of path exist\n",
        "    if os.path.exists(head_path):\n",
        "        #path last folder not exist\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "            return os.path.normpath(path)\n",
        "        #path last folder  exist\n",
        "        else:\n",
        "            if uniquify:\n",
        "                counter = 1\n",
        "                while os.path.exists(path):\n",
        "                    path = head_path + \"/\" + last_folder  + \"(\" + str(counter) + \")\"\n",
        "                    counter += 1\n",
        "                os.makedirs(path)\n",
        "            #empty last folder\n",
        "            elif clean:\n",
        "                if force:\n",
        "                    shutil.rmtree(path)\n",
        "                    os.makedirs(path)\n",
        "                else:\n",
        "                    print(f'Enter YES or Y to delete all file or directory from: {path}')\n",
        "                    input_clean = input()\n",
        "                    if input_clean in [\"YES\",\"Y\",\"yes\",\"y\"]:\n",
        "                        shutil.rmtree(path)\n",
        "                        os.makedirs(path)\n",
        "                    else:\n",
        "                        raise Util_class_folder_manage_forceDelete(path)\n",
        "        return os.path.normpath(path)\n",
        "    else:\n",
        "        print(f'Enter YES or Y to create directories: {path}')\n",
        "        input_clean = input()\n",
        "        if input_clean in [\"YES\",\"Y\",\"yes\",\"y\"]:\n",
        "            os.makedirs(path)\n",
        "            return os.path.normpath(path)\n",
        "        else:\n",
        "            raise Util_class_folder_manage_dirnameNotExist(head_path)\n",
        "\n",
        "class Util_class_folder_manage_dirnameNotExist(Exception):\n",
        "    \"\"\"Exception raised for errors in activation function type\"\"\"\n",
        "\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"Directory name '{self.value}' not exist.\"\n",
        "\n",
        "class Util_class_folder_manage_forceDelete(Exception):\n",
        "    \"\"\"Exception raised for errors in activation function type\"\"\"\n",
        "\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"Not possible force clean the folder: '{self.value}'.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NCBL96pIIcM"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "uLkBlb3Z7fIK"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "\n",
        "class LoadDataset():\n",
        "\n",
        "    def __init__(self,edge_file_name,attribute_file_name,label_file_name,attribute_file_format=\"normalized_matrix\",is_directed_graph=False):\n",
        "        \"\"\"\n",
        "        edge_file : file with all edge (pair of nodes)\n",
        "        attribute_file : file with all attribute\n",
        "        label_file :\n",
        "        attribute_file_format : format od attribute data:\n",
        "              \"normal_matrix\" : each row is alredy a frequency normalizzated vector (DEFAULT) es: CORA dataset\n",
        "              \"naive_text\" : each row is item text description\n",
        "        is_directed_graph : boolean, if true is a direct graph else (DEFAULT) is a undirect graph\n",
        "\n",
        "        \"\"\"\n",
        "        self.is_directed_graph = is_directed_graph\n",
        "        #input shape\n",
        "        self.input_shape = dict()\n",
        "\n",
        "        #Structural preprocessing\n",
        "        self.edge_file_name = edge_file_name\n",
        "        self.graph = self.edge_createGraph()\n",
        "        self.edge_adj_matrix = np.array(nx.to_numpy_array(self.graph, nodelist=sorted(self.graph.nodes())))\n",
        "\n",
        "        #Attribute preprocessing\n",
        "        self.attribute_file_name = attribute_file_name\n",
        "        self.attribute_adj_matrix = np.array(self.attribute_createMatrix(attribute_file_format))\n",
        "\n",
        "        #Class preprocessing\n",
        "        self.label_file_name = label_file_name\n",
        "        self.label_vec = self.labels_createVector()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_structural_matrix(self):\n",
        "        return self.edge_adj_matrix\n",
        "\n",
        "    def get_attribute_matrix(self):\n",
        "        return self.attribute_adj_matrix\n",
        "\n",
        "    def get_vector_matrix(self):\n",
        "        return self.label_vec\n",
        "\n",
        "    def get_graph(self):\n",
        "        return self.graph\n",
        "\n",
        "    def export_graph(self, pathfile, filename, extention=\"graphml\"):\n",
        "        path = pathfile+'/'+filename+'.'+extention\n",
        "\n",
        "        if extention == \"graphml\":\n",
        "            nx.write_graphml( self.graph, path)\n",
        "        elif extention == \"gml\":\n",
        "            nx.write_gml( self.graph, path)\n",
        "        else:\n",
        "            raise LoadDataset_Exception_Graph_FormatExport_notRecognized(extention)\n",
        "        return True\n",
        "\n",
        "    def get_input_shape(self, key):\n",
        "        return self.input_shape[key]\n",
        "\n",
        "    def edge_createGraph(self):\n",
        "        if self.is_directed_graph:\n",
        "            g = nx.DiGraph()\n",
        "        else:\n",
        "            g = nx.Graph()\n",
        "        try:\n",
        "            with open(self.edge_file_name, 'r') as edge_file:\n",
        "                for line in edge_file:\n",
        "                    edge = line.split()\n",
        "                    if len(edge) == 3:\n",
        "                        edge_weight = float(edge[2])\n",
        "                    else:\n",
        "                        edge_weight = 1.0\n",
        "                    if len(edge) == 1:\n",
        "                        g.add_node(int(edge[0]))\n",
        "                    else:\n",
        "                        g.add_edge(int(edge[0]), int(edge[1]), weight = edge_weight)\n",
        "        except Exception as e:\n",
        "            raise e\n",
        "        self.input_shape['net'] = g.number_of_nodes()\n",
        "        print(\"Structure dimension:\\t\",self.input_shape['net'])\n",
        "        return g\n",
        "\n",
        "    def attribute_createMatrix(self, attribute_file_format):\n",
        "        if attribute_file_format == \"normalized_matrix\":\n",
        "            try:\n",
        "                att_matrix = []\n",
        "                with open(self.attribute_file_name, 'r') as att_file:\n",
        "                    for line in att_file:\n",
        "                      att_line = line.replace(\"\\n\", \"\").split(\" \")[1:]\n",
        "                      att_matrix.append([float(n) for n in att_line])\n",
        "                self.input_shape['att'] = len(att_matrix[0])\n",
        "                print(\"Attribute dimension:\\t\",self.input_shape['att'])\n",
        "                return att_matrix\n",
        "            except Exception as e:\n",
        "                raise e\n",
        "        elif attribute_file_format == \"naive_text\":\n",
        "            print(\"naive_text to do\")\n",
        "            try:\n",
        "                att_matrix = []\n",
        "                with open(self.attribute_file_name, 'r') as att_file:\n",
        "                    for line in att_file:\n",
        "                        print(line)\n",
        "                        break\n",
        "                    corpus = json.load(att_file)\n",
        "                    print(corpus)\n",
        "\n",
        "                return 0\n",
        "            except Exception as e:\n",
        "                raise e\n",
        "        else:\n",
        "            raise(LoadDataset_Exception_Attribute_Format_notRecognized(attribute_file_format))\n",
        "\n",
        "\n",
        "    def labels_createVector(self):\n",
        "        try:\n",
        "            with open(self.label_file_name, 'r') as label_file:\n",
        "                node_label_dict = {}\n",
        "                for line in label_file:\n",
        "                    split_line = line.replace(\"\\n\", \"\").split(\" \")\n",
        "                    node_id = int(split_line[0])\n",
        "                    node_label = int(split_line[1])\n",
        "                    node_label_dict[node_id] = node_label\n",
        "                # sort the keys (node_ids) of the dictionary\n",
        "                node_label_dict = OrderedDict(sorted(node_label_dict.items(), key=lambda t: t[0]))\n",
        "                labels = np.array(list(node_label_dict.values()))\n",
        "                return labels\n",
        "        except Exception as e:\n",
        "            raise e\n",
        "\n",
        "    def get_labels(self):\n",
        "        return self.label_vec\n",
        "\n",
        "\n",
        "class LoadDataset_Exception_Attribute_Format_notRecognized(Exception):\n",
        "      \"\"\"Exception raised for errors in list of layers type\"\"\"\n",
        "\n",
        "      def __init__(self, value):\n",
        "          self.value = value\n",
        "\n",
        "      def __str__(self):\n",
        "          return f'{self.value} : type of attribute file format not recognized.'\n",
        "\n",
        "class LoadDataset_Exception_Graph_FormatExport_notRecognized(Exception):\n",
        "      \"\"\"Exception raised for errors in list of layers type\"\"\"\n",
        "\n",
        "      def __init__(self, value):\n",
        "          self.value = value\n",
        "\n",
        "      def __str__(self):\n",
        "          return f'{self.value} : graph format export not recognized.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbfAuKRKF1Nr"
      },
      "source": [
        "### Batch Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "q01sOY1MF1mY"
      },
      "outputs": [],
      "source": [
        "class DataBatchGenerator():\n",
        "\n",
        "    def __init__(self, net, att, labels, batch_size, shuffle, net_hadmard_coeff, att_hadmard_coeff):\n",
        "        self.net = net\n",
        "        self.att = att\n",
        "        self.labels = labels\n",
        "        self.number_of_samples = len(att)\n",
        "        self.batch_size = batch_size\n",
        "        self.number_of_batches = self.number_of_samples // batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.net_hadmard_coeff = net_hadmard_coeff\n",
        "        self.att_hadmard_coeff = att_hadmard_coeff\n",
        "\n",
        "    def generate(self):\n",
        "        sample_index = np.arange(self.net.shape[0])\n",
        "\n",
        "        counter = 0\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(sample_index)\n",
        "\n",
        "        while (counter*self.batch_size < self.number_of_samples):\n",
        "            start_samples_index = self.batch_size * counter\n",
        "            end_samples_index = self.batch_size * (counter + 1)\n",
        "\n",
        "            #list of samples's index\n",
        "            samples_index = sample_index[start_samples_index : end_samples_index]\n",
        "\n",
        "            #submatrix of W and A, cut for sample index\n",
        "            net_batch = self.net[samples_index, :]\n",
        "            att_batch = self.att[samples_index, :]\n",
        "            net_batch_adj = self.net[samples_index, :][:, samples_index]\n",
        "            node_label = self.labels[samples_index]\n",
        "            node_index = samples_index\n",
        "\n",
        "            # B_net and B_att param of hadmard operation\n",
        "            B_net = np.ones(net_batch.shape)\n",
        "            B_net[net_batch != 0] = self.net_hadmard_coeff\n",
        "\n",
        "            B_att = np.ones(att_batch.shape)\n",
        "            B_att[att_batch != 0] = self.att_hadmard_coeff\n",
        "\n",
        "            # trasform np array to tensor\n",
        "            net_batch_tensor = torch.from_numpy(net_batch).float()\n",
        "            att_batch_tensor = torch.from_numpy(att_batch).float()\n",
        "            net_batch_adj_tensor = torch.from_numpy(net_batch_adj).float()\n",
        "            B_net_tensor = torch.from_numpy(B_net).float()\n",
        "            B_att_tensor = torch.from_numpy(B_att).float()\n",
        "\n",
        "            inputs = [net_batch_tensor, att_batch_tensor, net_batch_adj_tensor]\n",
        "            B_params = [B_net_tensor, B_att_tensor]\n",
        "            batch_info = [node_index, node_label]\n",
        "\n",
        "            # feed the fit() function with new data\n",
        "            yield inputs, B_params, batch_info\n",
        "            counter += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqki-zA6Y7AF"
      },
      "source": [
        "### Competitive Kate Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "FnSf61oYZBvj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import warnings\n",
        "\n",
        "class KCompetitiveLayer(nn.Module):\n",
        "    \"\"\"\n",
        "      dim_imput :\n",
        "      act : String, activation function \"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ktop, alpha_factor):\n",
        "\n",
        "        super(KCompetitiveLayer, self).__init__()\n",
        "        self.ktop = ktop\n",
        "        self.alpha_factor = alpha_factor\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        dim_input = x.size()[1]\n",
        "        k = min(self.ktop, x.size(1))\n",
        "        if k < self.ktop:\n",
        "            warnings.warn(f\"ktop > input dim; using k={k} instead\")\n",
        "\n",
        "        # Posivite neurons computation\n",
        "        POS_ktop = int(self.ktop/2)\n",
        "        POS_values = (x + torch.abs(x))/2\n",
        "        POS_topk_values, POS_topk_indices = torch.topk(POS_values, k = POS_ktop)\n",
        "        device = x.device\n",
        "        batch_size = x.size(0)\n",
        "        POS_topk_range = torch.arange(batch_size, device=device).unsqueeze(1).repeat(1, POS_ktop)       \n",
        "        POS_full_indices = torch.reshape(torch.stack([POS_topk_range, POS_topk_indices], axis = 2), [-1, 2])\n",
        "        POS_sparse_values = torch.reshape(POS_topk_values, [-1])\n",
        "        POS_reset = torch.sparse_coo_tensor(indices = POS_full_indices.t(),values = POS_sparse_values, size = x.size()).to_dense()\n",
        "        POS_tmp = self.alpha_factor * torch.sum(POS_values - POS_reset, 1, keepdims=True)\n",
        "        POS_reset = torch.sparse_coo_tensor(indices = POS_full_indices.t(),values = torch.reshape(POS_topk_values+POS_tmp, [-1]), size = x.size()).to_dense()\n",
        "\n",
        "        # Negative neurons computation\n",
        "        NEG_ktop = self.ktop - int(self.ktop/2)\n",
        "        NEG_values = (x - torch.abs(x))/2\n",
        "        NEG_topk_values, NEG_topk_indices = torch.topk(-NEG_values,largest =True, k = NEG_ktop)\n",
        "        NEG_topk_range = torch.tile(torch.unsqueeze(torch.arange(0, NEG_topk_indices.size()[0]), 1), [1, NEG_ktop])\n",
        "        NEG_full_indices = torch.reshape(torch.stack([NEG_topk_range, NEG_topk_indices], axis = 2), [-1, 2])\n",
        "        NEG_sparse_values = torch.reshape(NEG_topk_values, [-1])\n",
        "        NEG_reset = torch.sparse_coo_tensor(indices = NEG_full_indices.t(),values = NEG_sparse_values, size = x.size()).to_dense()\n",
        "        NEG_tmp = self.alpha_factor * torch.sum(-NEG_values - NEG_reset, 1, keepdims=True)\n",
        "        NEG_reset = torch.sparse_coo_tensor(indices = NEG_full_indices.t(),values = torch.reshape(NEG_topk_values+NEG_tmp, [-1]), size = x.size()).to_dense()\n",
        "\n",
        "        # ensamble parts\n",
        "        total_reset = POS_reset - NEG_reset\n",
        "        return total_reset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otlEnyajIuru"
      },
      "source": [
        "### AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "0JKlbYBbD42f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    param dim: original dimension\n",
        "    param layers_list : sequential sort list of dict : Each item  have a value for \"type\", as:\n",
        "                                          DENSE -  hidden layers, with: \"features\" is dimention of features in output, \"act_funtion\" is the relative activation funcion,'bias' boolean\n",
        "                                          DROP  -  dropout, with: \"prob\" is the percentaul of neuro drop\n",
        "                                          KCOMP -  kcompetitivelayer,, with \"ktop\":int #of active neurons at end of computation, \"alpha_factor\":float coefficent\n",
        "    param latent_dim : dimension of latent space\n",
        "    last_isSigm : boolean, True if last activation function of decoder is a sigmoid\n",
        "    return : autoencoder model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, layers_list, latent_dim,last_isSigm= True):\n",
        "\n",
        "        super().__init__()\n",
        "        self.activation = {}\n",
        "        self.encoder_list=[]\n",
        "        self.decoder_list=[]\n",
        "\n",
        "        last_dim = dim\n",
        "        for i,layer in enumerate(layers_list):\n",
        "            if layer['type'] == \"DROP\":\n",
        "                prob = layer['prob']\n",
        "                if isinstance(prob, float) and 0 <= prob <= 1:\n",
        "                    self.encoder_list.append(torch.nn.Dropout(p=prob))\n",
        "                    self.decoder_list.insert(0,torch.nn.Dropout(p=prob))\n",
        "                else:\n",
        "                    raise AutoEncoder_Exception_DropoutProb(prob)\n",
        "\n",
        "            elif layer['type'] == \"DENSE\":\n",
        "                self.encoder_list.append(torch.nn.Linear(in_features=last_dim, out_features=layer['features'], bias=layer['bias']))\n",
        "                if layer['act_funtion'] == \"RELU\":\n",
        "                    self.encoder_list.append(torch.nn.ReLU())\n",
        "                    decoder_layer_funact = torch.nn.ReLU()\n",
        "                elif layer['act_funtion'] == \"SIGM\":\n",
        "                    self.encoder_list.append(torch.nn.Sigmoid())\n",
        "                    decoder_layer_funact = torch.nn.Sigmoid()\n",
        "                else:\n",
        "                    raise AutoEncoder_Exception_ActivationFunction(layer['act_funtion'])\n",
        "\n",
        "                if i == 0 and last_isSigm:\n",
        "                  decoder_layer_funact = torch.nn.Sigmoid()\n",
        "                self.decoder_list.insert(0,decoder_layer_funact)\n",
        "                self.decoder_list.insert(0,torch.nn.Linear(in_features=layer['features'], out_features=last_dim, bias=layer['bias']))\n",
        "                last_dim = layer['features']\n",
        "            elif layer['type'] == \"KCOMP\":\n",
        "                competitiveLayers = KCompetitiveLayer(layer['ktop'], layer['alpha_factor'])\n",
        "                self.encoder_list.append(competitiveLayers)\n",
        "            else:\n",
        "                raise AutoEncoder_Exception_Type(layer['type'])\n",
        "\n",
        "\n",
        "\n",
        "        if last_dim != latent_dim:\n",
        "            raise AutoEncoder_Exception_LatentSpace(last_dim,latent_dim)\n",
        "        self.encoder = nn.Sequential(*self.encoder_list)\n",
        "        self.decoder = nn.Sequential(*self.decoder_list)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x_latent = self.encoder(x)\n",
        "        x_hat = self.decoder(x_latent)\n",
        "        return {\"x_input\":x,\"x_latent\":x_latent,\"x_output\":x_hat}\n",
        "\n",
        "\n",
        "class AutoEncoder_Exception_Type(Exception):\n",
        "      \"\"\"Exception raised for errors in list of layers type\"\"\"\n",
        "\n",
        "      def __init__(self, value):\n",
        "          self.value = value\n",
        "\n",
        "      def __str__(self):\n",
        "          return f'{self.value} : type layer not recognized: it should be a hidden layer linear (DENSE) or dropout layer (DROP).'\n",
        "\n",
        "\n",
        "class AutoEncoder_Exception_DropoutProb(Exception):\n",
        "      \"\"\"Exception raised for errors in list of layers type\"\"\"\n",
        "\n",
        "      def __init__(self, value):\n",
        "          self.value = value\n",
        "\n",
        "      def __str__(self):\n",
        "          if isinstance(self.value, float):\n",
        "              return f'Dropout should have probability param in range 0 to 1, but receive {self.value}.'\n",
        "          else:\n",
        "              return f'Dropout should be a float but receive a {type(self.value)}.'\n",
        "\n",
        "\n",
        "\n",
        "class AutoEncoder_Exception_ActivationFunction(Exception):\n",
        "      \"\"\"Exception raised for errors in activation function type\"\"\"\n",
        "\n",
        "      def __init__(self, value):\n",
        "          self.value = value\n",
        "\n",
        "      def __str__(self):\n",
        "          return f'{self.value} : activation function not recognized: it should be a relu function (RELU), a sigmoid funcion (SIGM).'\n",
        "\n",
        "\n",
        "\n",
        "class AutoEncoder_Exception_LatentSpace(Exception):\n",
        "      \"\"\"Exception raised for errors in list of layers type: last layer in list haven't the latent space dimention\"\"\"\n",
        "\n",
        "      def __init__(self, last_dim,latent_dim):\n",
        "          self.last_dim = last_dim\n",
        "          self.latent_dim = latent_dim\n",
        "\n",
        "      def __str__(self):\n",
        "          return f'Last layer have {self.last_dim} output dimention but latent space should be {self.latent_dim}.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zAeQRL6I1dI"
      },
      "source": [
        "### Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "-pUMkV9qhkaq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "\n",
        "all_a = None\n",
        "all_b = None\n",
        "class LossFunction(nn.Module):\n",
        "    \"\"\"\n",
        "    loss_functions : list of dictionary:\n",
        "            loss_name : loss function name,\n",
        "            coef : coefficent to totla loss function\n",
        "    matrix_values : dictionary\n",
        "            net : structure matrix\n",
        "                y_true : grountruth matrix\n",
        "                y_late : embedding matrix\n",
        "                y_pred : predict matrix\n",
        "             att : semantical matrix\n",
        "                y_true : grountruth  matrix\n",
        "                y_late : embedding matrix\n",
        "                y_pred : predict matrix\n",
        "    \"\"\"\n",
        "    def __init__(self, loss_functions,matrix_values):\n",
        "      self.loss_functions = loss_functions\n",
        "      self.matrix_values = matrix_values\n",
        "\n",
        "    def loss_computate(self,verbose=False):\n",
        "\n",
        "        loss_total = torch.zeros(1)\n",
        "        for loss_function in self.loss_functions:\n",
        "            loss_name = loss_function['loss_name']\n",
        "\n",
        "            coef = loss_function['coef']\n",
        "            if isinstance(coef, float) or isinstance(coef, int):\n",
        "                if loss_name == \"structur_proximity_1order\":\n",
        "                    _val = self.structur_proximity_1order( self.matrix_values[\"net\"][\"y_late\"], self.matrix_values[\"net\"][\"y_adj\"],self.matrix_values[\"net\"])\n",
        "                    loss_total.add_(_val.mul(coef))\n",
        "                elif loss_name == \"semantic_proximity_1order\":\n",
        "                    _val = self.semantic_proximity_1order( self.matrix_values[\"att\"][\"y_late\"], self.matrix_values[\"net\"][\"y_adj\"])\n",
        "                    loss_total.add_(_val.mul(coef))\n",
        "                elif loss_name == \"structur_proximity_2order\":\n",
        "                    _val = self.structur_proximity_2order( self.matrix_values[\"net\"][\"y_pred\"], self.matrix_values[\"net\"][\"y_true\"], self.matrix_values[\"net\"][\"B_param\"])\n",
        "                    loss_total.add_(_val.mul(coef))\n",
        "                elif loss_name == \"semantic_proximity_2order\":\n",
        "                    _val = self.semantic_proximity_2order( self.matrix_values[\"att\"][\"y_pred\"], self.matrix_values[\"att\"][\"y_true\"], self.matrix_values[\"att\"][\"B_param\"])\n",
        "                    loss_total.add_(_val.mul(coef))\n",
        "                elif loss_name == \"consisency_proximity\":\n",
        "                    _val = self.consisency_proximity( self.matrix_values[\"net\"][\"y_late\"], self.matrix_values[\"att\"][\"y_late\"])\n",
        "                    loss_total.add_(_val.mul(coef))\n",
        "                elif loss_name == \"consisency_compl_proximity\":\n",
        "                    _val = self.consisency_compl_proximity( self.matrix_values[\"net\"][\"y_late\"], self.matrix_values[\"att\"][\"y_late\"])\n",
        "                    loss_total.add_(_val.mul(coef))\n",
        "                elif loss_name == \"square_diff_embedding_proximity\":\n",
        "                    _val = self.square_diff_embedding_proximity( self.matrix_values[\"net\"][\"y_late\"], self.matrix_values[\"att\"][\"y_late\"], self.matrix_values[\"net\"][\"y_adj\"])\n",
        "                    loss_total.add_(_val.mul(coef))\n",
        "                else:\n",
        "                    raise LossFunction_Exception_FuntionNotExist(loss_name)\n",
        "            else:\n",
        "                raise LossFunction_Exception_Coeff(loss_name, coef)\n",
        "            if verbose:\n",
        "                print(\"\\t\",loss_name,\"\\t->\\t\",_val,\"\\tTOT:\\t\",loss_total)\n",
        "        if verbose:\n",
        "            print(\"----\\tTOTAL:\\t\",loss_total,\"\\t\\t----\\n\")\n",
        "        return loss_total\n",
        "\n",
        "\n",
        "    def structur_proximity_1order(self, hs_emb, w_matrix,oth = None):\n",
        "        \"\"\"\n",
        "        hs_emb : embedding matrix\n",
        "        w_matrix : structural adjacency matrix\n",
        "        return a tensor with value 0\n",
        "        \"\"\"\n",
        "        sigmoid_argument = torch.matmul(hs_emb,torch.transpose(hs_emb,0,1))\n",
        "        labels_1 = w_matrix + torch.eye(w_matrix.size()[0])\n",
        "        cross_E1 = self.__sigmoid_cross_entropy_with_logits(labels= labels_1, logits= sigmoid_argument,inp=sigmoid_argument,oth=oth)\n",
        "        labels_2 = torch.ones_like(torch.diag(sigmoid_argument))\n",
        "        logits_2 = torch.diag(sigmoid_argument)\n",
        "        cross_E2 = self.__sigmoid_cross_entropy_with_logits(labels= labels_2, logits= logits_2,oth=oth)\n",
        "        cross_All = cross_E1 - cross_E2\n",
        "        return torch.mean(cross_All)\n",
        "\n",
        "    def semantic_proximity_1order(self, hs_emb, w_matrix):\n",
        "        \"\"\"\n",
        "        hs_emb : embedding matrix\n",
        "        w_matrix : structural adjacency matrix\n",
        "        return a tensor with value 0\n",
        "        \"\"\"\n",
        "        sigmoid_argument = torch.matmul(hs_emb,torch.transpose(hs_emb,0,1))\n",
        "        labels_1 = w_matrix + torch.eye(w_matrix.size()[0])\n",
        "        cross_E1 = self.__sigmoid_cross_entropy_with_logits(labels= labels_1, logits= sigmoid_argument)\n",
        "        labels_2 = torch.ones_like(torch.diag(sigmoid_argument))\n",
        "        logits_2 = torch.diag(sigmoid_argument)\n",
        "        cross_E2 = self.__sigmoid_cross_entropy_with_logits(labels= labels_2, logits= logits_2)\n",
        "        cross_All = cross_E1 - cross_E2\n",
        "        return torch.mean(cross_All)\n",
        "\n",
        "    def structur_proximity_2order(self, ys_true, ys_pred, b_param):\n",
        "        \"\"\"\n",
        "        ys_true : vector of items where each item is a groundtruth matrix\n",
        "        ys_pred : vector of items where each item is a prediction matrix\n",
        "        return the sum of 2nd proximity of 2 matrix\n",
        "        \"\"\"\n",
        "        loss_secondary = 0\n",
        "\n",
        "        for i, y_true in enumerate(ys_true):\n",
        "            y_pred = ys_pred[i]\n",
        "\n",
        "            loss_secondary_item = torch.norm(torch.square(torch.sub(y_pred,y_true,alpha=1) * b_param), p=2)\n",
        "            loss_secondary += loss_secondary_item\n",
        "        return loss_secondary\n",
        "\n",
        "    def semantic_proximity_2order(self, ys_true, ys_pred, b_param):\n",
        "        \"\"\"\n",
        "        ys_true : vector of items where each item is a groundtruth matrix\n",
        "        ys_pred : vector of items where each item is a prediction matrix\n",
        "        return the sum of 2nd proximity of 2 matrix\n",
        "        \"\"\"\n",
        "        loss_secondary = 0\n",
        "\n",
        "        for i, y_true in enumerate(ys_true):\n",
        "            y_pred = ys_pred[i]\n",
        "            loss_secondary_item = torch.norm(torch.square(torch.sub(y_pred,y_true,alpha=1) * b_param), p=2)\n",
        "            loss_secondary += loss_secondary_item\n",
        "        return loss_secondary\n",
        "\n",
        "    def consisency_proximity(self, hs_net, hs_att):\n",
        "        \"\"\"\n",
        "        hs_net : matrix embedding structure\n",
        "        hs_att : matrix embedding attribute\n",
        "        return the consisency proximity value\n",
        "        \"\"\"\n",
        "        loss_secondary = 0\n",
        "\n",
        "        for i, h_net in enumerate(hs_net):\n",
        "            h_att = hs_att[i]\n",
        "            loss_secondary_item = torch.norm(torch.square(torch.sub(h_att,h_net,alpha=1)), p=2)\n",
        "            loss_secondary += loss_secondary_item\n",
        "        return loss_secondary\n",
        "\n",
        "\n",
        "    def consisency_compl_proximity(self, hs_net, hs_att):\n",
        "        \"\"\"\n",
        "        hs_net : matrix embedding structure\n",
        "        hs_att : matrix embedding attribute\n",
        "        return the consisency proximity value\n",
        "        \"\"\"\n",
        "\n",
        "        logits = torch.sum(torch.multiply(hs_net, hs_att), dim=1)\n",
        "        labels = torch.ones_like(logits)\n",
        "        cross_All = self.__sigmoid_cross_entropy_with_logits(labels= labels, logits= logits)\n",
        "        return torch.mean(cross_All)\n",
        "\n",
        "    def __softmax_cross_entropy_with_logits(self, labels, logits):\n",
        "        _cross_entropy = -torch.sum(F.log_softmax(logits, dim=1) * labels, dim=1)\n",
        "        return _cross_entropy\n",
        "\n",
        "    def __sigmoid_cross_entropy_with_logits(self, labels, logits,inp=None, oth=None):\n",
        "        eps = 1e-12\n",
        "        _cross_entropy_a = (labels * -torch.log(torch.sigmoid(logits) + eps))\n",
        "        _cross_entropy_b = (1 - labels) * - torch.log(1 - torch.sigmoid(logits) + eps)\n",
        "        _cross_entropy = _cross_entropy_a + _cross_entropy_b\n",
        "        return _cross_entropy\n",
        "\n",
        "    def square_diff_embedding_proximity(self, hs_net, hs_att, w_matrix):\n",
        "        \"\"\"\n",
        "        ys_true : vector of items where each item is a groundtruth matrix\n",
        "        ys_pred : vector of items where each item is a prediction matrix\n",
        "        return the sum of 2nd proximity of 2 matrix\n",
        "        \"\"\"\n",
        "\n",
        "        struct_proximity = self.structur_proximity_1order(hs_net, w_matrix)\n",
        "        attrib_proximity = self.semantic_proximity_1order(hs_att, w_matrix)\n",
        "        loss_square = torch.square(attrib_proximity + torch.neg(struct_proximity))\n",
        "        return loss_square\n",
        "\n",
        "\n",
        "\n",
        "class LossFunction_Exception_Coeff(Exception):\n",
        "      \"\"\"Exception raised for error if coeff is not int or float\"\"\"\n",
        "\n",
        "      def __init__(self, loss_name, value):\n",
        "          self.value = value\n",
        "          self.loss_name = loss_name\n",
        "\n",
        "      def __str__(self):\n",
        "          return f'Loss \"{self.loss_name}\" coefficent should be a float or int but receive a {type(self.value)}.'\n",
        "\n",
        "class LossFunction_Exception_FuntionNotExist(Exception):\n",
        "      \"\"\"Exception raised for error if coeff is not int or float\"\"\"\n",
        "\n",
        "      def __init__(self, loss_name):\n",
        "          self.loss_name = loss_name\n",
        "\n",
        "      def __str__(self):\n",
        "          return f'Loss \"{self.loss_name}\" not exist.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KppIu4dSEp9f"
      },
      "source": [
        "### Ottimizzatore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "B0qoGygFEqSP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "#import AutoEncoder\n",
        "#import Loss_function\n",
        "\n",
        "class OptimizationFunction():\n",
        "\n",
        "    def __init__(self, opt_config):\n",
        "        \"\"\"\n",
        "          opt_config : list of dictionary:\n",
        "              opt_name : optimizator function name,\n",
        "              lr_rate :learning rate\n",
        "              weight_decay : [OPT - if adam_L2] decay weight param\n",
        "        \"\"\"\n",
        "        self.name_opt = opt_config[\"opt_name\"]\n",
        "        self.lr_rate = opt_config[\"lr_rate\"]\n",
        "        if self.name_opt not in [\"adam\", \"adam_L2\"]:\n",
        "            raise OptimizationFunction_Exception_OptimizatorNotExist(self.name_opt)\n",
        "\n",
        "        if self.name_opt == \"adam_L2\":\n",
        "            if \"weight_decay\" not in opt_config:\n",
        "                raise OptimizationFunction_OptimizatorParamsMissing(self.name_opt,\"weight_decay\")\n",
        "            else:\n",
        "                self.weight_decay = opt_config[\"weight_decay\"]\n",
        "\n",
        "\n",
        "    def get_optimizator(self, net_model):\n",
        "        self.net_params = net_model.parameters()\n",
        "        if self.name_opt == \"adam\":\n",
        "            return torch.optim.Adam(params=self.net_params, lr=self.lr_rate)\n",
        "        elif self.name_opt == \"adam_L2\":\n",
        "            return torch.optim.Adam(params=self.net_params, lr=self.lr_rate, weight_decay=self.weight_decay)\n",
        "        else:\n",
        "            raise OptimizationFunction_Exception_OptimizatorNotExist(self.name_opt)\n",
        "\n",
        "\"\"\"\n",
        "class OptimizationFunction():\n",
        "  optimizer = optim.SGD([torch.rand((2,2), requires_grad=True)], lr=0.1)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\"\"\"\n",
        "\n",
        "class OptimizationFunction_Exception_OptimizatorParamsMissing(Exception):\n",
        "      \"\"\"Exception raised for error if a param is missing\"\"\"\n",
        "\n",
        "      def __init__(self, name_opt, name_param_missing):\n",
        "          self.name_opt = name_opt\n",
        "          self.name_param_missing = name_param_missing\n",
        "\n",
        "      def __str__(self):\n",
        "          return f'Optimizator \"{self.name_opt}\" needs \"{self.name_param_missing}\" param.'\n",
        "\n",
        "class OptimizationFunction_Exception_OptimizatorNotExist(Exception):\n",
        "      \"\"\"Exception raised for error if optimizator not exist\"\"\"\n",
        "\n",
        "      def __init__(self, opt_name):\n",
        "          self.opt_name = opt_name\n",
        "\n",
        "      def __str__(self):\n",
        "          return f'Optimizator \"{self.opt_name}\" not exist.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_u48wicRIzb"
      },
      "source": [
        " ###  Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "EwMWaaXmRGea"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "#import AutoEncoder\n",
        "#import Loss_function\n",
        "\n",
        "class RegularizationFunction():\n",
        "\n",
        "    def __init__(self, reg_config):\n",
        "        \"\"\"\n",
        "          reg_config : list of dictionary:\n",
        "              reg_name : regularization name,\n",
        "              coeff : regularization coefficent\n",
        "\n",
        "        \"\"\"\n",
        "        self.regularizations = reg_config\n",
        "\n",
        "\n",
        "    def get_regularization(self, net_model):\n",
        "\n",
        "        net_params = net_model.parameters()\n",
        "        loss_reg = 0\n",
        "        for _reg in self.regularizations:\n",
        "            reg_lambda = _reg[\"coeff\"]\n",
        "            reg_name = _reg[\"reg_name\"]\n",
        "            if reg_name == \"L1\":\n",
        "                reg_norm1 = sum(param.abs().sum() for param in net_params)\n",
        "                loss_reg += reg_lambda * reg_norm1\n",
        "            elif reg_name == \"L2\":\n",
        "                reg_norm2 = sum(param.pow(2.0).sum() for param in net_params)\n",
        "                loss_reg += reg_lambda * reg_norm2\n",
        "            else:\n",
        "                raise RegularizationFunction_Exception_RegularizationNotExist(reg_name)\n",
        "        return loss_reg\n",
        "\n",
        "\n",
        "\n",
        "class RegularizationFunction_Exception_RegularizationNotExist(Exception):\n",
        "      \"\"\"Exception raised for error if optimizator not exist\"\"\"\n",
        "\n",
        "      def __init__(self, opt_name):\n",
        "          self.opt_name = opt_name\n",
        "\n",
        "      def __str__(self):\n",
        "          return f'Regularization \"{self.opt_name}\" not exist.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdKXGPdtYGog"
      },
      "source": [
        "\n",
        "\n",
        "### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "q6vfDaYNYK0P"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "class CheckpointModel():\n",
        "    def __init__(self, save_config):\n",
        "      \"\"\"\n",
        "        save_config : dict\n",
        "            type : [\"best_model_loss\", \"every_tot\", \"first_train\", \"last_train\"]\n",
        "            times : [OPT - if type is \"every_tot\"] int, number of epoch when save\n",
        "            overwrite : boolean\n",
        "            path_file : path where save\n",
        "            name_file : name of file\n",
        "            path_not_exist_mode: if path not exist: \"create\",\"except\", default:except\n",
        "            path_exist_mode: \"use\",\"clean\",\"new\" default:\"use\"\n",
        "      \"\"\"\n",
        "      self.type_checkpointer = [\"best_model_loss\", \"every_tot\", \"first_train\", \"last_train\"]\n",
        "      self.checher = dict()\n",
        "      if save_config == None:\n",
        "          self.checher['enable'] = False\n",
        "      else:\n",
        "          self.checher['enable'] = True\n",
        "          is_types_safe = True\n",
        "          not_types_safe = list()\n",
        "\n",
        "          for type_config in save_config['type']:\n",
        "            if type_config not in self.type_checkpointer:\n",
        "              is_types_safe = False\n",
        "              not_types_safe.append(type_config)\n",
        "\n",
        "          if is_types_safe:\n",
        "              self.checher[\"type\"] = save_config['type']\n",
        "\n",
        "              dirpath_save = save_config[\"path_file\"]\n",
        "              self.checher[\"name_file\"] = save_config[\"name_file\"]\n",
        "\n",
        "              if \"path_not_exist\" in save_config:\n",
        "                  path_not_exist_mode = save_config[\"path_not_exist\"]\n",
        "              else:\n",
        "                  path_not_exist_mode = \"except\"\n",
        "\n",
        "              if \"path_exist\" in save_config:\n",
        "                  path_exist_mode = save_config[\"path_exist\"]\n",
        "              else:\n",
        "                  path_exist_mode = \"use\"\n",
        "\n",
        "\n",
        "              if os.path.isdir(dirpath_save): #path esiste\n",
        "                  if path_exist_mode == \"use\":\n",
        "                      self.checher[\"path_file\"] = dirpath_save\n",
        "                  elif path_exist_mode == \"clean\":\n",
        "                      self.checher[\"path_file\"] = Util_class.folder_manage(dirpath_save, clean=True)\n",
        "                  elif path_exist_mode == \"new\":\n",
        "                      self.checher[\"path_file\"] = Util_class.folder_manage(dirpath_save, uniquify=True)\n",
        "                  else:\n",
        "                      raise CheckpointModel_Exception_ParamPathNotRecoignezed(\"Exist\", path_exist_mode)\n",
        "\n",
        "              else: #path non esiste\n",
        "                  if path_not_exist_mode == \"create\":\n",
        "                      self.checher[\"path_file\"] = Util_class.folder_manage(dirpath_save, uniquify=True)\n",
        "                  elif path_not_exist_mode == \"except\":\n",
        "                      raise CheckpointModel_Exception_SavePathNotExist(dirpath_save)\n",
        "                  else:\n",
        "                      raise CheckpointModel_Exception_ParamPathNotRecoignezed(\"NotExist\", path_exist_mode)\n",
        "\n",
        "              print(\"Your model's checkpoint is save in : {fpath}\".format(fpath = self.checher[\"path_file\"]))\n",
        "\n",
        "              if \"overwrite\" in save_config:\n",
        "                  self.checher[\"overwrite\"] = save_config[\"overwrite\"]\n",
        "              else:\n",
        "                  self.checher[\"overwrite\"] = False\n",
        "\n",
        "              for type_config in save_config['type']:\n",
        "                  if type_config == \"every_tot\":\n",
        "                      self.checher[\"times\"] = save_config['times']\n",
        "                      self.checher[\"next_epoch\"] = save_config['times']\n",
        "                  elif type_config == \"best_model_loss\":\n",
        "                      self.checher[\"last_loss\"] = None\n",
        "          else:\n",
        "              raise CheckpointModel_Exception_TypeChecker(not_types_safe)\n",
        "\n",
        "    def checkToSave(self,graphe_model,epoch,epochs,loss, phase=None):\n",
        "        if not isinstance(graphe_model, GraphEModel):\n",
        "            raise CheckpointModel_Exception_GraphEModelType(graphe_model)\n",
        "        else:\n",
        "          to_save = False\n",
        "\n",
        "          for type_config in self.checher['type']:\n",
        "              if type_config == \"every_tot\":\n",
        "                  if self.checher['next_epoch'] == epoch:\n",
        "                      to_save = True\n",
        "                      self.checher['next_epoch'] += self.checher[\"times\"]\n",
        "              elif type_config == \"best_model_lost\":\n",
        "                  if self.checher['last_loss'] > loss:\n",
        "                      to_save = True\n",
        "                      self.checher['last_loss'] = loss\n",
        "              elif type_config == \"first_train\":\n",
        "                  if epoch==1:\n",
        "                    to_save = True\n",
        "              elif type_config == \"last_train\":\n",
        "                  if epoch == epochs:\n",
        "                    to_save = True\n",
        "\n",
        "          if phase is not None:\n",
        "              _phase = \"_phase\" + phase\n",
        "          else:\n",
        "              _phase = \"\"\n",
        "          if to_save:\n",
        "              if self.checher[\"overwrite\"]:\n",
        "                  path_chechpoint_file = \"{fpath}/{fname}{fphase}.carbo\".format(fpath = self.checher[\"path_file\"], fname = self.checher[\"name_file\"], fphase=_phase)\n",
        "              else:\n",
        "                  for type_config in self.checher['type']:\n",
        "                      if type_config in [\"every_tot\", \"first_train\", \"last_train\"]:\n",
        "                          path_chechpoint_file = \"{fpath}/{fname}{fphase}_epoch_{fepoch}.carbo\".format(fpath = self.checher[\"path_file\"], fname = self.checher[\"name_file\"], fepoch = epoch, fphase=_phase)\n",
        "                      elif type_config in [\"best_model_lost\"]:\n",
        "                          path_chechpoint_file = \"{fphase}_epoch_{fepoch}_loss_{floss:.8f}.carbo\".format(fpath = self.checher[\"path_file\"],fname = self.checher[\"name_file\"], fepoch = epoch, floss = loss, fphase=_phase)\n",
        "              graphe_model.save_model(epoch = epoch, path_file = path_chechpoint_file)\n",
        "              print(\"Epoch : \",epoch,\"/\",epochs,\"\\tLoss : \", loss, \"\\tmodel checkpoint saved as: {fpath}\".format(fpath=path_chechpoint_file))\n",
        "\n",
        "\n",
        "class CheckpointModel_Exception_TypeChecker(Exception):\n",
        "      \"\"\"Exception raised for errors in activation function type\"\"\"\n",
        "\n",
        "      def __init__(self, value):\n",
        "          self.value = value\n",
        "\n",
        "      def __str__(self):\n",
        "          return f'{self.value} : type of checkpointer not recognized.'\n",
        "\n",
        "class CheckpointModel_Exception_GraphEModelType(Exception):\n",
        "      \"\"\"Exception raised for errors in activation function type\"\"\"\n",
        "\n",
        "      def __init__(self, value):\n",
        "          self.value = type(value)\n",
        "\n",
        "      def __str__(self):\n",
        "          return f'Model should be a GraphEModel object but checker receiver a {self.value} type object.'\n",
        "\n",
        "class CheckpointModel_Exception_SavePathNotExist(Exception):\n",
        "      \"\"\"Exception raised for errors in activation function type\"\"\"\n",
        "\n",
        "      def __init__(self, path):\n",
        "          self.path = path\n",
        "\n",
        "      def __str__(self):\n",
        "          return f\"Your model's checkpoint could be save because '{self.path}' not exist.\"\n",
        "\n",
        "class CheckpointModel_Exception_ParamPathNotRecoignezed(Exception):\n",
        "      \"\"\"Exception raised for errors of path to save embedding is none\"\"\"\n",
        "\n",
        "      def __init__(self,mode,value):\n",
        "          self.value = value\n",
        "          self.mode = mode\n",
        "\n",
        "      def __str__(self):\n",
        "          return f\"{self.mode} modality param {self.value} is recognized.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjmJCzGZJI-u"
      },
      "source": [
        "### Modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "e7_KEPNsUMyU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchviz import make_dot\n",
        "import _pickle as cPickle\n",
        "# import Util_class\n",
        "# import AutoEncoder\n",
        "# import Loss_function\n",
        "\n",
        "class GraphEModel(nn.Module):\n",
        "\n",
        "    def __init__(self, model_config,):\n",
        "        \"\"\"\n",
        "          model_config : dictionary:\n",
        "              att_dim : dimension of input and output of attribute/semantical space\n",
        "              att_layers_list : param layers_list : sequential sort list of semantical network architecture\n",
        "              att_latent_dim : dimension of embedding/latent semantical space\n",
        "\n",
        "              net_dim : dimension of input and output of structural/network space\n",
        "              net_layers_list : param layers_list : sequential sort list of structural network architecture\n",
        "              net_latent_dim : dimension of embedding/latent structural space\n",
        "\n",
        "              loss_functions : dictionary key: all,net,att\n",
        "                  - all : A+N training modality, same loss for both\n",
        "                  - net : A/N>N/A training modality, set loss for net model\n",
        "                  - att : A/N>N/A training modality, set loss for att model\n",
        "                  Each item is a list of vector: [Loss_function function name, param]\n",
        "\n",
        "              regularization_net : list of dictionary of regularization for structure\n",
        "                      reg_name : regularization function name,\n",
        "                      coeff : coeff regularization influence\n",
        "\n",
        "              regularization_att : list of dictionary of regularization for semantical\n",
        "                      reg_name : regularization function name,\n",
        "                      coeff : coeff regularization influence\n",
        "\n",
        "\n",
        "              model_name : string, name of model\n",
        "\n",
        "              optimizator_net : dictionary - optimizator config for structure\n",
        "                      opt_name : optimizator function name,\n",
        "                      lr_rate :learning rate\n",
        "                      weight_decay : [OPT - if adam_L2] decay weight param\n",
        "\n",
        "              optimizator_att : dictionary - optimizator config for semantical\n",
        "                      opt_name : optimizator function name,\n",
        "                      lr_rate :learning rate\n",
        "                      weight_decay : [OPT - if adam_L2] decay weight param\n",
        "\n",
        "              checkpoint_config : configuration for checkpoint\n",
        "\n",
        "              training_config : string, order to make a training\n",
        "                  \"A>N\" : first attribute and then structure\n",
        "                  \"N>A\" : first structure and then attribute\n",
        "                  \"A+N\" : attribute and structure simultaneously\n",
        "                  \"N+A\" : attribute and structure simultaneously\n",
        "        \"\"\"\n",
        "        super(GraphEModel, self).__init__()\n",
        "        self.epochs_status = dict()\n",
        "\n",
        "        self.att_dim = model_config[\"att_dim\"]\n",
        "        self.att_layers_list = model_config[\"att_layers_list\"]\n",
        "        self.att_latent_dim = model_config[\"att_latent_dim\"]\n",
        "        self.epochs_status['att'] = 0\n",
        "\n",
        "        self.net_dim = model_config[\"net_dim\"]\n",
        "        self.net_layers_list = model_config[\"net_layers_list\"]\n",
        "        self.net_latent_dim = model_config[\"net_latent_dim\"]\n",
        "        self.epochs_status['net'] = 0\n",
        "\n",
        "        self.loss_functions = model_config[\"loss_functions\"]\n",
        "\n",
        "        self.model_name = model_config[\"model_name\"]\n",
        "\n",
        "        # Model Autoencoders Initialization\n",
        "        self.autoEncoder = dict()\n",
        "        self.autoEncoder['att'] = AutoEncoder(dim=self.att_dim, layers_list=self.att_layers_list, latent_dim=self.att_latent_dim)\n",
        "        self.autoEncoder['net'] = AutoEncoder(dim=self.net_dim, layers_list=self.net_layers_list, latent_dim=self.net_latent_dim)\n",
        "\n",
        "        #Optimization Initialization\n",
        "        self.optimizatior = dict()\n",
        "        opt_net_obj = OptimizationFunction(model_config['optimizator_net'])\n",
        "        opt_att_obj = OptimizationFunction(model_config['optimizator_att'])\n",
        "        self.optimizatior['net'] = opt_net_obj.get_optimizator(self.autoEncoder['net'])\n",
        "        self.optimizatior['att'] = opt_att_obj.get_optimizator(self.autoEncoder['att'])\n",
        "\n",
        "        #Regularization Initialization\n",
        "        self.regularization = dict()\n",
        "        regularization_net_obj = RegularizationFunction(model_config['regularization_net'])\n",
        "        regularization_att_obj = RegularizationFunction(model_config['regularization_att'])\n",
        "        self.regularization['net'] = regularization_net_obj\n",
        "        self.regularization['att'] = regularization_att_obj\n",
        "\n",
        "        #self.optimizatior['net'] = torch.optim.Adam(params=self.autoEncoder['net'].parameters(), lr=1e-3,weight_decay=1e-4)\n",
        "        #self.optimizatior['att'] = torch.optim.Adam(params=self.autoEncoder['att'].parameters(), lr=1e-3,weight_decay=1e-4)\n",
        "\n",
        "        self.scheduler = dict()\n",
        "        #self.scheduler['net'] = optim.lr_scheduler.StepLR(opt_net, step_size=15, gamma=0.5)\n",
        "        #self.scheduler['att'] = optim.lr_scheduler.StepLR(opt_att, step_size=15, gamma=0.5)\n",
        "        self.scheduler['net'] = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizatior['net'], mode='min',factor=0.1, patience=5)\n",
        "        self.scheduler['att'] = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizatior['att'], mode='min',factor=0.1, patience=5)\n",
        "\n",
        "        self.checkpointer = CheckpointModel(model_config[\"checkpoint_config\"])\n",
        "        self.training_config = model_config['training_config']\n",
        "\n",
        "        self.space_embedded = { 'net': dict(), 'att': dict(), 'node_label':dict()}\n",
        "\n",
        "\n",
        "    def get_Model_semantical(self):\n",
        "        return self.autoEncoder['att']\n",
        "\n",
        "    def get_Model_structural(self):\n",
        "        return self.autoEncoder['net']\n",
        "\n",
        "    def get_Models(self):\n",
        "        return {\"att\":self.get_Model_semantical(), \"net\":self.get_Model_structural()}\n",
        "\n",
        "    def save_model(self, epoch, path_file):\n",
        "        torch.save({\n",
        "            'NET_model_state_dict': self.autoEncoder['net'].state_dict(),\n",
        "            'ATT_model_state_dict': self.autoEncoder['att'].state_dict(),\n",
        "\n",
        "            'NET_optimizer_state_dict': self.optimizatior['net'].state_dict(),\n",
        "            'ATT_optimizer_state_dict': self.optimizatior['att'].state_dict(),\n",
        "\n",
        "            'epochs_status': self.epochs_status,\n",
        "            'space_embedded': self.space_embedded,\n",
        "            'checkpointer': self.checkpointer,\n",
        "\n",
        "          }, path_file)\n",
        "\n",
        "    def load_model(self, path_file):\n",
        "        checkpoint = torch.load(path_file)\n",
        "        self.autoEncoder['net'].load_state_dict(checkpoint['NET_model_state_dict'])\n",
        "        self.autoEncoder['att'].load_state_dict(checkpoint['ATT_model_state_dict'])\n",
        "\n",
        "        self.optimizatior['net'].load_state_dict(checkpoint['NET_optimizer_state_dict'])\n",
        "        self.optimizatior['att'].load_state_dict(checkpoint['ATT_optimizer_state_dict'])\n",
        "\n",
        "        self.epochs_status = checkpoint['epochs_status']\n",
        "        self.space_embedded = checkpoint['space_embedded']\n",
        "        self.checkpointer = checkpoint['checkpointer']\n",
        "\n",
        "    def save_embedding(self):\n",
        "        raise NotImplementedError('GraphE save_embedding not implemented')\n",
        "\n",
        "    def model_info(self):\n",
        "        print(\"STRUCTURAL Model's state_dict :\")\n",
        "        for param_tensor in self.autoEncoder['net'].state_dict():\n",
        "            print(param_tensor, \"\\t\", self.autoEncoder['net'].state_dict()[param_tensor].size())\n",
        "\n",
        "        print(\"SEMANTICAL Model's state_dict :\")\n",
        "        for param_tensor in self.autoEncoder['att'].state_dict():\n",
        "            print(param_tensor, \"\\t\", self.autoEncoder['att'].state_dict()[param_tensor].size())\n",
        "\n",
        "        # Print optimizer's state_dict\n",
        "        print(\"STRUCTURAL Optimizer's state_dict:\")\n",
        "        for var_name in self.optimizatior['net'].state_dict():\n",
        "            print(var_name, \"\\t\", self.optimizatior['net'].state_dict()[var_name])\n",
        "        print(\"SEMANTICAL Optimizer's state_dict:\")\n",
        "        for var_name in self.optimizatior['att'].state_dict():\n",
        "            print(var_name, \"\\t\", self.optimizatior['att'].state_dict()[var_name])\n",
        "\n",
        "    def models_training(self, datagenerator, epochs, path_embedding=\"/content/\", loss_verbose=False):\n",
        "\n",
        "\n",
        "        if (self.training_config == \"A+N\") or (self.training_config == \"N+A\"):\n",
        "\n",
        "            if isinstance(epochs, dict):\n",
        "              res = self.models_training_simultaneously(datagenerator, epochs, path_embedding= path_embedding, loss_verbose= loss_verbose)\n",
        "\n",
        "            else:\n",
        "              raise GraphEModel_Exception__TrainingEpochType(epochs,self.training_config,int)\n",
        "\n",
        "        elif self.training_config == \"A>N\":\n",
        "            phases = [\"att\",\"net\"]\n",
        "            if isinstance(epochs, dict):\n",
        "              epochs_check =Util_class.check_key_in_dict(epochs,phases)\n",
        "              if epochs_check[0]:\n",
        "                  res = self.models_training_2phased(phases,datagenerator, epochs, path_embedding= path_embedding, loss_verbose= loss_verbose)\n",
        "              else:\n",
        "                raise GraphEModel_Exception__TrainingEpochItems(epochs,phases,epochs_check[1])\n",
        "            else:\n",
        "              raise GraphEModel_Exception__TrainingEpochType(epochs,self.training_config,dict)\n",
        "\n",
        "        elif self.training_config == \"N>A\":\n",
        "            phases = [\"net\",\"att\"]\n",
        "            if isinstance(epochs, dict) :\n",
        "              epochs_check =Util_class.check_key_in_dict(epochs,phases)\n",
        "              if epochs_check[0]:\n",
        "                  res = self.models_training_2phased(phases,datagenerator, epochs, path_embedding= path_embedding, loss_verbose= loss_verbose)\n",
        "              else:\n",
        "                raise GraphEModel_Exception__TrainingEpochItems(epochs,phases,epochs_check[1])\n",
        "            else:\n",
        "              raise GraphEModel_Exception__TrainingEpochType(epochs,self.training_config,dict)\n",
        "\n",
        "        else:\n",
        "            raise GraphEModel_Exception__TrainingModality()\n",
        "        return res\n",
        "\n",
        "    def models_training_simultaneously(self, datagenerator, epochs, path_embedding=\"/content/\", loss_verbose=False):\n",
        "        \"\"\"\n",
        "        data : DataBatchGenerator, data\n",
        "        epochs : int, times re-training process do\n",
        "        \"\"\"\n",
        "        outputs = dict()\n",
        "        losses = []\n",
        "\n",
        "        if not isinstance(datagenerator, DataBatchGenerator):\n",
        "            raise GraphEModel_Exception__notDataBatchGeneratorClass(datagenerator)\n",
        "\n",
        "\n",
        "\n",
        "        epochs_time = epochs[\"all\"]\n",
        "\n",
        "        tot_epochs = epochs_time + self.epochs_status['att']\n",
        "\n",
        "        for epoch in range(1, epochs_time+1):\n",
        "\n",
        "\n",
        "            loss_epoch =  []\n",
        "            if epoch %2 == 0:\n",
        "                print(\"=\")\n",
        "            else:\n",
        "                print(\"==\")\n",
        "\n",
        "\n",
        "            node_4batch = list()\n",
        "\n",
        "            for [input, B_param, batch_info] in datagenerator.generate():\n",
        "                [net_batch, att_batch, net_batch_adj_tensor] = input\n",
        "                [B_net, B_att] = B_param\n",
        "                [node_index, node_labels] = batch_info\n",
        "\n",
        "                # Output of Autoencoder\n",
        "                net_comp = self.autoEncoder['net'].forward(net_batch)\n",
        "                att_comp = self.autoEncoder['att'].forward(att_batch)\n",
        "\n",
        "                # Calculating the loss function\n",
        "                loss_values_matrix = {\n",
        "                    \"net\": {\n",
        "                        \"y_true\" : net_comp[\"x_input\"],\n",
        "                        \"y_late\" : net_comp[\"x_latent\"],\n",
        "                        \"y_pred\" : net_comp[\"x_output\"],\n",
        "                        \"B_param\" : B_net,\n",
        "                        \"y_adj\" : net_batch_adj_tensor,\n",
        "                    },\n",
        "                    \"att\": {\n",
        "                        \"y_true\" : att_comp[\"x_input\"],\n",
        "                        \"y_late\" : att_comp[\"x_latent\"],\n",
        "                        \"y_pred\" : att_comp[\"x_output\"],\n",
        "                        \"B_param\": B_att,\n",
        "                        \"y_adj\" : None,\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                loss_obj = LossFunction(self.loss_functions['all'], loss_values_matrix)\n",
        "                loss = loss_obj.loss_computate(loss_verbose)\n",
        "                if torch.isnan(loss):\n",
        "                    print(loss_values_matrix)\n",
        "                    raise NotImplementedError('loss is nan')\n",
        "\n",
        "                regularization_influence_net = self.regularization['net'].get_regularization(self.autoEncoder['net'])\n",
        "                regularization_influence_att = self.regularization['att'].get_regularization(self.autoEncoder['att'])\n",
        "                regularization_loss = regularization_influence_net + regularization_influence_att\n",
        "\n",
        "                loss += regularization_loss\n",
        "                # Resetta il gradiente\n",
        "                self.optimizatior['net'].zero_grad()\n",
        "                self.optimizatior['att'].zero_grad()\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                # The gradients are set to zero,\n",
        "                # the the gradient is computed and stored.\n",
        "                # .step() performs parameter update\n",
        "                self.optimizatior['net'].step()\n",
        "                self.optimizatior['att'].step()\n",
        "\n",
        "\n",
        "                # Storing the losses in a list for plotting\n",
        "                #losses.append(loss)\n",
        "                loss_epoch.append(loss.item())\n",
        "                loss_mean_epoch = sum(loss_epoch) / float(len(loss_epoch))\n",
        "\n",
        "                if epoch == tot_epochs-1:\n",
        "                    output_dict_net = {\n",
        "                        \"latent\" : net_comp[\"x_latent\"],\n",
        "                        \"output\" : net_comp[\"x_output\"],\n",
        "                    }\n",
        "                    output_dict_att = {\n",
        "                        \"input\" : att_comp[\"x_input\"],\n",
        "                        \"latent\" : att_comp[\"x_latent\"],\n",
        "                        \"output\" : att_comp[\"x_output\"],\n",
        "                    }\n",
        "                    node_info = {\n",
        "                        \"node_index\": node_index,\n",
        "                        \"node_label\" : node_labels,\n",
        "                    }\n",
        "\n",
        "                    output_dict = {\n",
        "                        \"net\" : output_dict_net,\n",
        "                        \"att\" : output_dict_att,\n",
        "                        \"node_info\":node_info\n",
        "                    }\n",
        "                    node_4batch.append(output_dict)\n",
        "\n",
        "\n",
        "            self.epochs_status['att'] += 1\n",
        "            self.epochs_status['net'] += 1\n",
        "            epoch_globaly = self.epochs_status['net']\n",
        "\n",
        "            self.scheduler['net'].step(loss_mean_epoch)\n",
        "            self.scheduler['att'].step(loss_mean_epoch)\n",
        "            losses.append(loss_mean_epoch)\n",
        "            print(\"Epoch : \",epoch_globaly,\"/\",tot_epochs,\"\\tLoss : \",loss_mean_epoch,\"\\tlr net: \",self.optimizatior['net'].param_groups[0]['lr'],\"\\tlr att: \",self.optimizatior['att'].param_groups[0]['lr'])\n",
        "\n",
        "            #pointchecker save a model according by checkpointer config\n",
        "            self.checkpointer.checkToSave(self, epoch_globaly,tot_epochs, loss_mean_epoch)\n",
        "\n",
        "\n",
        "            outputs[epoch] = node_4batch\n",
        "\n",
        "        self.set_embedding(encoder_out=outputs, last_epoch= epochs_time - 1, save=True, path=path_embedding, phases=['att', 'net'])\n",
        "        return {\"output\":outputs, \"losses\":losses, \"saved_embedding\":True}\n",
        "\n",
        "    def models_training_2phased(self, phases_list,datagenerator, epochs, path_embedding=\"/content/\", loss_verbose=False):\n",
        "        check_phase = Util_class.same_key_in_dict(phases_list, ['net','att'])\n",
        "\n",
        "        if not check_phase[0]:\n",
        "            raise GraphEModel_Exception__TrainingPhasesNotSame(check_phase)\n",
        "\n",
        "        outputs = dict()\n",
        "        losses = dict()\n",
        "        prev_phase_embedding = {\n",
        "            'net' : {'index' : None,  'latent' : None},\n",
        "            'att' : {'index' : None,  'latent' : None},\n",
        "        }\n",
        "\n",
        "        if not isinstance(datagenerator, DataBatchGenerator):\n",
        "            raise GraphEModel_Exception__notDataBatchGeneratorClass(datagenerator)\n",
        "\n",
        "        for phase in phases_list:\n",
        "            epochs_time = epochs[phase]\n",
        "            tot_epochs = epochs_time + self.epochs_status[phase]\n",
        "            losses[phase] = list()\n",
        "\n",
        "            if epochs[phase] < 1:\n",
        "                print(f\"No epoch to train for phase: {phase}.\")\n",
        "\n",
        "            for epoch in range(epochs_time):\n",
        "\n",
        "                loss_epoch =  []\n",
        "\n",
        "                if epoch %2 == 0:\n",
        "                    print(\"=\")\n",
        "                else:\n",
        "                    print(\"==\")\n",
        "\n",
        "                node_4batch = list()\n",
        "\n",
        "                for [input, B_param, batch_info] in datagenerator.generate():\n",
        "                    [net_batch, att_batch, net_batch_adj_tensor] = input\n",
        "                    [B_net, B_att] = B_param\n",
        "                    [node_index, node_labels] = batch_info\n",
        "\n",
        "                    # Output of Autoencoder\n",
        "                    autoencoder_component = dict()\n",
        "                    if phase == \"net\":\n",
        "                        autoencoder_component['net'] = self.autoEncoder['net'].forward(net_batch)\n",
        "                        loss_values_matrix = {\n",
        "                            \"net\": {\n",
        "                                \"y_true\" : autoencoder_component['net'][\"x_input\"],\n",
        "                                \"y_late\" : autoencoder_component['net'][\"x_latent\"],\n",
        "                                \"y_pred\" : autoencoder_component['net'][\"x_output\"],\n",
        "                                \"B_param\" : B_net,\n",
        "                                \"y_adj\" : net_batch_adj_tensor,\n",
        "                            },\n",
        "                            \"att\": {\n",
        "                                \"y_true\" : None,#autoencoder_component['att'][\"x_input\"],\n",
        "                                \"y_late\" : None,#autoencoder_component['att'][\"x_latent\"],\n",
        "                                \"y_pred\" : None,#autoencoder_component['att'][\"x_output\"],\n",
        "                                \"B_param\": None,#B_att,\n",
        "                                \"y_adj\" : None,\n",
        "                            }\n",
        "                        }\n",
        "                    elif phase == \"att\":\n",
        "                        autoencoder_component['att'] = self.autoEncoder['att'].forward(att_batch)\n",
        "                        loss_values_matrix = {\n",
        "                            \"net\": {\n",
        "                                \"y_true\" : None,#autoencoder_component['net'][\"x_input\"],\n",
        "                                \"y_late\" : self.get_embedding(nodes_list=node_index, phase='net', type_output='tensor'),#autoencoder_component['net'][\"x_latent\"],\n",
        "                                \"y_pred\" : None,#autoencoder_component['net'][\"x_output\"],\n",
        "                                \"B_param\" : B_net,\n",
        "                                \"y_adj\" : net_batch_adj_tensor,\n",
        "                            },\n",
        "                            \"att\": {\n",
        "                                \"y_true\" : autoencoder_component['att'][\"x_input\"],\n",
        "                                \"y_late\" : autoencoder_component['att'][\"x_latent\"],\n",
        "                                \"y_pred\" : autoencoder_component['att'][\"x_output\"],\n",
        "                                \"B_param\": B_att,\n",
        "                                \"y_adj\" : None,\n",
        "                            }\n",
        "                        }\n",
        "                    # Calculating the loss function\n",
        "\n",
        "\n",
        "                    '''\n",
        "                    if phase == 'net':\n",
        "                        loss_values_matrix[phase][\"B_param\"] = B_net\n",
        "                        loss_values_matrix[phase][\"y_adj\"] = net_batch_adj_tensor\n",
        "                        loss_values_matrix['att'] = {}\n",
        "                        loss_values_matrix['att']['x_latent'] = None\n",
        "\n",
        "                    elif phase == 'att':\n",
        "                        loss_values_matrix[phase][\"B_param\"] = B_att\n",
        "                        loss_values_matrix[phase][\"y_adj\"] = None\n",
        "                        loss_values_matrix['net'] = {}\n",
        "                        loss_values_matrix['net']['x_latent'] = None\n",
        "                    '''\n",
        "\n",
        "\n",
        "                    loss_obj = LossFunction(self.loss_functions[phase], loss_values_matrix)\n",
        "\n",
        "                    loss = loss_obj.loss_computate(loss_verbose)\n",
        "\n",
        "\n",
        "\n",
        "                    regularization_influence = self.regularization[phase].get_regularization(self.autoEncoder[phase])\n",
        "                    regularization_loss = regularization_influence\n",
        "\n",
        "                    loss += regularization_loss\n",
        "\n",
        "                    # Reset gradient\n",
        "                    self.optimizatior[phase].zero_grad()\n",
        "\n",
        "\n",
        "                    #if phase == 'att':\n",
        "                    #    make_dot(loss).render(\"loss\", format=\"png\")\n",
        "\n",
        "\n",
        "                    loss.backward(retain_graph=True)\n",
        "                    # The gradients are set to zero,\n",
        "                    # the the gradient is computed and stored.\n",
        "                    # .step() performs parameter update\n",
        "                    self.optimizatior[phase].step()\n",
        "\n",
        "                    # Storing the losses in a list for plotting\n",
        "                    #losses.append(loss)\n",
        "                    loss_epoch.append(loss.item())\n",
        "\n",
        "                    # yhat\n",
        "                    #make_dot(yhat, params=dict(list(model.named_parameters()))).render(\"rnn_torchviz\", format=\"png\")\n",
        "                    #make_dot(autoencoder_component['att']['x_latent']).render(\"att_x_latent\", format=\"png\")\n",
        "                    #make_dot(autoencoder_component['att']['x_output']).render(\"att_x_output\", format=\"png\")\n",
        "                    #make_dot(loss).render(\"att_loss\", format=\"png\")\n",
        "\n",
        "                    if epoch == epochs_time-1:\n",
        "                        output_dict = {\n",
        "                            \"input\" : autoencoder_component[phase][\"x_input\"],\n",
        "                            \"latent\" : autoencoder_component[phase][\"x_latent\"],\n",
        "                            \"output\" : autoencoder_component[phase][\"x_output\"],\n",
        "                        }\n",
        "                        node_info = {\n",
        "                            \"node_index\": node_index,\n",
        "                            \"node_label\" : node_labels,\n",
        "                        }\n",
        "\n",
        "                        output_dict = {\n",
        "                            phase : output_dict,\n",
        "                            \"node_info\":node_info\n",
        "                        }\n",
        "                        node_4batch.append(output_dict)\n",
        "\n",
        "                loss_mean_epoch = sum(loss_epoch) / float(len(loss_epoch))\n",
        "\n",
        "                self.epochs_status[phase] += 1\n",
        "                epoch_globaly = self.epochs_status[phase]\n",
        "\n",
        "                self.scheduler[phase].step(loss_mean_epoch)\n",
        "                losses[phase].append(loss_mean_epoch)\n",
        "                print(\"Phase : \",phase,\"\\tEpoch : \",epoch_globaly,\"/\",tot_epochs,\"\\tLoss : \",loss_mean_epoch,\"\\tlr net: \",self.optimizatior['net'].param_groups[0]['lr'],\"\\tlr att: \",self.optimizatior['att'].param_groups[0]['lr'])\n",
        "\n",
        "                #raise NotImplementedError('GraphE models_training_phased not implemented')\n",
        "                self.checkpointer.checkToSave(self, epoch_globaly,tot_epochs, loss_mean_epoch,phase=phase)\n",
        "            outputs[phase] = node_4batch\n",
        "            self.set_embedding(encoder_out=outputs, last_epoch= phase, save=True, path=path_embedding, phases=[phase])\n",
        "        return {\"output\":outputs, \"losses\":losses, \"saved_embedding\":True}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #\"\"\"  prev_phase_embedding[phase]['index'] =\n",
        "    #prev_phase_embedding[phase]['latent'] =\n",
        "    #\"\"\"\n",
        "\n",
        "    def get_embedding(self, nodes_list=None, phase='net', type_output='tensor'):\n",
        "\n",
        "        if phase not in ['net','att','node_label']:\n",
        "            raise GraphEModel_Exception__EmbeddingKeyNotRecoignezed(phase)\n",
        "        if nodes_list is None or len(nodes_list) == 0:\n",
        "            nodes_list = []\n",
        "            for k in self.space_embedded['node_label']:\n",
        "                nodes_list.append(k)\n",
        "            if len(nodes_list)>0:\n",
        "                return self.get_embedding(nodes_list,phase,type_output)\n",
        "            else:\n",
        "                raise GraphEModel_Exception__EmbeddingNodeIdNotFound(-1)\n",
        "        else:\n",
        "            embedding_request = None\n",
        "            for node_id in nodes_list:\n",
        "                if node_id not in self.space_embedded['node_label']:\n",
        "                    raise GraphEModel_Exception__EmbeddingNodeIdNotFound(node_id)\n",
        "                else:\n",
        "                    if phase == 'node_label':\n",
        "                        if embedding_request is None:\n",
        "                            embedding_request = [self.space_embedded[phase][node_id]]\n",
        "                        else:\n",
        "                            embedding_request.append(self.space_embedded[phase][node_id])\n",
        "\n",
        "                    else:\n",
        "                        if embedding_request is None:\n",
        "                            embedding_request = self.space_embedded[phase][node_id]\n",
        "                        else:\n",
        "                            embedding_request = torch.vstack([embedding_request,self.space_embedded[phase][node_id]])\n",
        "\n",
        "            if type_output == 'tensor':\n",
        "                return embedding_request\n",
        "            elif (type_output == 'np' or type_output == 'numpy') and phase == 'node_label':\n",
        "                return np.array(embedding_request)\n",
        "            elif (type_output == 'np' or type_output == 'numpy'):\n",
        "                return np.array(list(embedding_request.detach().numpy()))\n",
        "            else:\n",
        "                raise GraphEModel_Exception__EmbeddingNodeIdNotFound(node_id)\n",
        "\n",
        "            return embedding_request\n",
        "\n",
        "    def set_embedding(self, encoder_out, last_epoch, save=False, path=None, phases=[\"net\",\"att\"]):\n",
        "        \"\"\"\n",
        "        batches : epoch batches\n",
        "        epoch : int, epoch to analized embedding\n",
        "\n",
        "        RETURN set locally embedding space selected and if save=True it is saved in a file\n",
        "        \"\"\"\n",
        "\n",
        "        for phase in phases:\n",
        "            print(\"Set embedding for:\\t\",phase)\n",
        "            for batch in range(len(encoder_out[last_epoch])):\n",
        "                  for i in range(len(encoder_out[last_epoch][batch]['node_info']['node_index'])):\n",
        "                      node_key = encoder_out[last_epoch][batch]['node_info']['node_index'][i]\n",
        "                      self.space_embedded[phase][node_key] = encoder_out[last_epoch][batch][phase]['latent'][i].data.clone()\n",
        "\n",
        "                      self.space_embedded['node_label'][node_key] = encoder_out[last_epoch][batch]['node_info']['node_label'][i]\n",
        "            if save:\n",
        "                if path is None:\n",
        "                    raise GraphEModel_Exception__notPathEmbeddingParam()\n",
        "                else:\n",
        "                    path_embedding_file = \"{fpath}embedding_{fmodelname}_{fphase}.ecarbo\".format(fpath = path, fmodelname = self.model_name, fphase=phase)\n",
        "                    with open(path_embedding_file, \"wb\") as fileEmbedding:\n",
        "                          cPickle.dump(self.space_embedded, fileEmbedding)\n",
        "                    print(f\"Saved embedding for:\\t {phase}\\t\\t on path:\\t{path_embedding_file}\")\n",
        "\n",
        "class GraphEModel_Exception__notDataBatchGeneratorClass(Exception):\n",
        "      \"\"\"Exception raised for errors of data input type\"\"\"\n",
        "\n",
        "      def __init__(self, value):\n",
        "          self.value = value\n",
        "\n",
        "      def __str__(self):\n",
        "          return f\"{type(self.value)} : type of attribute file format not recognized. It should be a 'DataBatchGenerator' istance.\"\n",
        "\n",
        "class GraphEModel_Exception__notPathEmbeddingParam(Exception):\n",
        "      \"\"\"Exception raised for errors of path to save embedding is none\"\"\"\n",
        "\n",
        "      def __init__(self):\n",
        "          self.value = None\n",
        "\n",
        "      def __str__(self):\n",
        "          return f\"Path where save embedding is None.\"\n",
        "\n",
        "class GraphEModel_Exception__EmbeddingKeyNotRecoignezed(Exception):\n",
        "      \"\"\"Exception raised for errors of path to save embedding is none\"\"\"\n",
        "\n",
        "      def __init__(self,value):\n",
        "          self.value = value\n",
        "\n",
        "      def __str__(self):\n",
        "          return f\"{self.value} is not embedding recognized key. Phase accept are: 'net','att' and 'node_label'.\"\n",
        "class GraphEModel_Exception__EmbeddingNodeIdNotFound(Exception):\n",
        "      \"\"\"Exception raised for errors of path to save embedding is none\"\"\"\n",
        "\n",
        "      def __init__(self,value):\n",
        "          self.value = value\n",
        "\n",
        "      def __str__(self):\n",
        "          return f\"Node id '{self.value}' not found.\"\n",
        "\n",
        "class GraphEModel_Exception__TrainingModality(Exception):\n",
        "      \"\"\"Exception raised for error no training modality recognized\"\"\"\n",
        "\n",
        "      def __init__(self,value):\n",
        "          self.value = None\n",
        "\n",
        "      def __str__(self):\n",
        "          return f\"{self.value} is not a modality for training recognized. It should be: 'A+N' or 'N<A' or 'A>N'.\"\n",
        "\n",
        "class GraphEModel_Exception__TrainingEpochType(Exception):\n",
        "      \"\"\"Exception raised for error no training modality recognized\"\"\"\n",
        "\n",
        "      def __init__(self,value,modality,typeObjRequest):\n",
        "          self.value = value\n",
        "          self.modality = modality\n",
        "          self.typeObjRequest = typeObjRequest\n",
        "\n",
        "      def __str__(self):\n",
        "          return f\"In modality of training like {self.modality}, epoch value shoud be a {str(self.typeObjRequest)} object but receive an {str(type(self.value))} object.\"\n",
        "\n",
        "class GraphEModel_Exception__TrainingEpochItems(Exception):\n",
        "      \"\"\"Exception raised for error no training modality recognized\"\"\"\n",
        "\n",
        "      def __init__(self,value, keyRequest, keyMissing):\n",
        "          self.value = value\n",
        "          self.keyRequest = keyRequest\n",
        "          self.keyMissing = (\" \").join(keyMissing)\n",
        "\n",
        "      def __str__(self):\n",
        "          return f\"Epochs array should have {len(self.keyRequest)} items but receive {len(self.value)} items. Key Missin is: {self.keyMissing}\"\n",
        "\n",
        "class GraphEModel_Exception__TrainingPhasesNotSame(Exception):\n",
        "      \"\"\"Exception raised for error no training modality recognized\"\"\"\n",
        "\n",
        "      def __init__(self,value,list_check_phases):\n",
        "          self.value = value\n",
        "          self.is_same = list_check_phases[0]\n",
        "          self.key_not_dict = list_check_phases[1]\n",
        "          self.key_not_list = list_check_phases[2]\n",
        "\n",
        "      def __str__(self):\n",
        "          message = \"Phase should be same of declaration but:\\n\"\n",
        "          if len(self.key_not_dict) > 0:\n",
        "              _msg = (\" \").join(self.key_not_dict)\n",
        "              message += f\"There are input phases key not recognized:\\n\\t {_msg} \\n\"\n",
        "\n",
        "          if len(self.key_not_list) > 0:\n",
        "              _msg = (\" \").join(self.key_not_list)\n",
        "              message += f\"There are  missing phases:\\n\\t {_msg} \\n\"\n",
        "\n",
        "          return message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mSXrquB_0bZ"
      },
      "source": [
        "*testo in corsivo*### Node Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "bHKottFrAY1y"
      },
      "outputs": [],
      "source": [
        "import functools as ft\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, rand_score\n",
        "from sklearn import model_selection as sk_ms\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn import preprocessing\n",
        "\n",
        "class NodeClassification():\n",
        "\n",
        "    def __init__(self, embedding_data, labels, normalize=False):\n",
        "        if normalize:\n",
        "            self.data = pd.DataFrame(preprocessing.normalize(embedding_data, norm='l2', axis=1))\n",
        "        else:\n",
        "            self.data = pd.DataFrame(embedding_data)\n",
        "        self.oper_math = ['sum', 'avg']\n",
        "        self.labels = labels\n",
        "        self.performal_measure = {\n",
        "            \"accuracy_score\" : ft.partial(accuracy_score),\n",
        "            \"precision_macro\" : ft.partial(precision_score, average='macro'),\n",
        "            \"precision_micro\" : ft.partial(precision_score, average='micro'),\n",
        "            \"recall_macro\" : ft.partial(recall_score, average='macro'),\n",
        "            \"recall_micro\" : ft.partial(recall_score, average='micro'),\n",
        "            \"f1_macro\" : ft.partial(f1_score, average='macro', labels=np.unique(self.labels)),\n",
        "            \"f1_micro\" : ft.partial(f1_score, average='micro', labels=np.unique(self.labels)),\n",
        "\n",
        "            \"precision_weighted\" : ft.partial(precision_score, average='weighted'),\n",
        "            \"recall_weighted\" : ft.partial(recall_score, average='weighted'),\n",
        "            \"f1_weighted\" : ft.partial(f1_score, average='weighted', labels=np.unique(self.labels)),\n",
        "        }\n",
        "\n",
        "    def split_dataset(self, split_threshold, num_split, random_set):\n",
        "        \"\"\"\n",
        "        split_threshold : float, threshold of test split\n",
        "        num_split : int, number of split\n",
        "        random_set : boolean, TRUE if each split is randomly different by others\n",
        "        return : array, where each item is a dictionary of data's split, with keys X_train, X_test, Y_train, Y_test\n",
        "        \"\"\"\n",
        "        data_splitted = list()\n",
        "        _splitted_data = None\n",
        "        for i in range(num_split):\n",
        "            if random_set or (_splitted_data == None):\n",
        "                X_train, X_test, Y_train, Y_test = sk_ms.train_test_split(self.data, self.labels, test_size=split_threshold)\n",
        "                _splitted_data = {\n",
        "                    \"X_train\" : X_train,\n",
        "                    \"Y_train\" : Y_train,\n",
        "                    \"X_test\" : X_test,\n",
        "                    \"Y_test\" : Y_test,\n",
        "                }\n",
        "            data_splitted.append(_splitted_data)\n",
        "        return data_splitted\n",
        "\n",
        "\n",
        "\n",
        "    def classification(self, classifier_name, split_threshold, repetitions, group_by, random_set):\n",
        "        \"\"\"\n",
        "        classifier_name : string, name of classifier. Implemented \"svm\",\n",
        "        split_threshold : float, threshold of test split\n",
        "        repetitions : int, number of repetitions of classification\n",
        "        random_set : boolean, TRUE if each split is randomly different by others\n",
        "        group_by : array, math operation group by measure\n",
        "        return : array, each item is a data's split\n",
        "        \"\"\"\n",
        "        data_splitted = self.split_dataset(split_threshold, repetitions, random_set)\n",
        "\n",
        "        if classifier_name == \"svm\":\n",
        "            classifier = LinearSVC()\n",
        "        else:\n",
        "            raise NodeClassification_notClassifierFound(classifier_name)\n",
        "        measures_performance = dict()\n",
        "\n",
        "        for n_repetition in range(repetitions):\n",
        "            data_split = data_splitted.pop(0)\n",
        "            classifier.fit(data_split[\"X_train\"], data_split[\"Y_train\"])\n",
        "            predictions = self.prediction(classifier, data_split[\"X_test\"])\n",
        "            performance_computation = self.performance_computation(data_split[\"Y_test\"], predictions)\n",
        "\n",
        "            key_iter = \"iter_\"+str(n_repetition)\n",
        "            measures_performance[key_iter] = performance_computation\n",
        "\n",
        "        measure_total = dict()\n",
        "        for oper in group_by:\n",
        "            if oper in self.oper_math:\n",
        "                measure_total[oper] = dict()\n",
        "                for measure_name in self.performal_measure:\n",
        "                      measure_total[oper][measure_name] = list()\n",
        "            else:\n",
        "                raise NodeClassification_notAggregationRecognizer(oper)\n",
        "\n",
        "        for iteration_measure in measures_performance:\n",
        "            for measure_name in self.performal_measure:\n",
        "                value = measures_performance[iteration_measure][measure_name]\n",
        "                for oper in group_by:\n",
        "                    measure_total[oper][measure_name].append(value)\n",
        "        for oper in group_by:\n",
        "            for measure_name in self.performal_measure:\n",
        "                if oper == \"avg\":\n",
        "                    measure_total[oper][measure_name] = sum(measure_total[oper][measure_name])/len(measure_total[oper][measure_name])\n",
        "                elif oper == \"sum\":\n",
        "                    measure_total[oper][measure_name] = sum(measure_total[oper][measure_name])\n",
        "                else:\n",
        "                    measure_total[oper][measure_name] = 0\n",
        "        return measure_total\n",
        "\n",
        "\n",
        "    def prediction(self, model, data):\n",
        "        return model.predict(data)\n",
        "\n",
        "    def performance_computation(self, Y_test, Y_pred):\n",
        "        performance_measure_computed = dict()\n",
        "\n",
        "        for measure_name in self.performal_measure:\n",
        "            measure_function = self.performal_measure[measure_name]\n",
        "            measure_value = measure_function(Y_test, Y_pred)\n",
        "            performance_measure_computed[measure_name] = measure_value\n",
        "        return performance_measure_computed\n",
        "\n",
        "\n",
        "class NodeClassification_notClassifierFound(Exception):\n",
        "      \"\"\"Exception raised for not classifier type found\"\"\"\n",
        "\n",
        "      def __init__(self, name):\n",
        "          self.name = name\n",
        "\n",
        "      def __str__(self):\n",
        "          return f\"{type(self.name)} : type of classifier not found.\"\n",
        "\n",
        "class NodeClassification_notAggregationRecognizer(Exception):\n",
        "      \"\"\"Exception raised for not classifier type found\"\"\"\n",
        "\n",
        "      def __init__(self, name):\n",
        "          self.name = name\n",
        "\n",
        "      def __str__(self):\n",
        "          return f\" Classification not support '{type(self.name)}' group_by. It's accept: 'sum' or 'avg'.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrN99AyKeXbW"
      },
      "source": [
        "### Node Clustering\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "xxJyVKdYeXbh"
      },
      "outputs": [],
      "source": [
        "import functools as ft\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, rand_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from nltk.cluster.util import cosine_distance, euclidean_distance\n",
        "from nltk.cluster.kmeans import KMeansClusterer\n",
        "\n",
        "class NodeClustering():\n",
        "\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.oper_math = ['sum', 'avg']\n",
        "        self.labels = labels\n",
        "\n",
        "        self.performal_measure = {\n",
        "            \"rand_score\" : ft.partial(rand_score),\n",
        "            #\"davies_bouldin_score\" : ft.partial(davies_bouldin_score),\n",
        "            #\"calinski_harabasz_score\" : ft.partial(calinski_harabasz_score),\n",
        "        }\n",
        "\n",
        "    def classic_clusterizzation(self, repetitions, group_by):\n",
        "        \"\"\"\n",
        "        repetitions : int, number of repetitions of classification\n",
        "        group_by : array, math operation group by measure\n",
        "        return : array, each item is a data's split\n",
        "        \"\"\"\n",
        "\n",
        "        classic_centroid_number = len(np.unique(self.labels))\n",
        "\n",
        "        measures_performance = dict()\n",
        "\n",
        "        for n_repetition in range(repetitions):\n",
        "            kmeans = KMeansClusterer(classic_centroid_number, distance=cosine_distance, normalise=True, avoid_empty_clusters=True)\n",
        "            Y_pred = kmeans.cluster(self.data, assign_clusters=True)\n",
        "            performance_computation = self.performance_computation(self.labels, Y_pred)\n",
        "            key_iter = \"iter_\"+str(n_repetition)\n",
        "            measures_performance[key_iter] = performance_computation\n",
        "        measure_total = dict()\n",
        "        for oper in group_by:\n",
        "            if oper in self.oper_math:\n",
        "                measure_total[oper] = dict()\n",
        "                for measure_name in self.performal_measure:\n",
        "                    if measure_name not in self.performal_measure:\n",
        "                        raise NodeClustering_notPerformanceRecognizer(measure_name)\n",
        "                    else:\n",
        "                        measure_total[oper][measure_name] = list()\n",
        "            else:\n",
        "                raise NodeClustering_notAggregationRecognizer(oper)\n",
        "\n",
        "        for iteration_measure in measures_performance:\n",
        "            for measure_name in self.performal_measure:\n",
        "                value = measures_performance[iteration_measure][measure_name]\n",
        "                for oper in group_by:\n",
        "                    measure_total[oper][measure_name].append(value)\n",
        "        for oper in group_by:\n",
        "            for measure_name in self.performal_measure:\n",
        "                if oper == \"avg\":\n",
        "                    measure_total[oper][measure_name] = sum(measure_total[oper][measure_name])/len(measure_total[oper][measure_name])\n",
        "                elif oper == \"sum\":\n",
        "                    measure_total[oper][measure_name] = sum(measure_total[oper][measure_name])\n",
        "                else:\n",
        "                    measure_total[oper][measure_name] = 0\n",
        "        return measure_total\n",
        "\n",
        "    def performance_computation(self, Y_test, Y_pred):\n",
        "        performance_measure_computed = dict()\n",
        "\n",
        "        for measure_name in self.performal_measure:\n",
        "            measure_function = self.performal_measure[measure_name]\n",
        "            measure_value = measure_function(Y_test, Y_pred)\n",
        "            performance_measure_computed[measure_name] = measure_value\n",
        "\n",
        "        return performance_measure_computed\n",
        "\n",
        "class NodeClustering_notAggregationRecognizer(Exception):\n",
        "      \"\"\"Exception raised for not classifier type found\"\"\"\n",
        "\n",
        "      def __init__(self, name):\n",
        "          self.name = name\n",
        "\n",
        "      def __str__(self):\n",
        "          return f\" Clustering not support '{type(self.name)}' group_by. It's accept: 'sum' or 'avg'.\"\n",
        "\n",
        "class NodeClustering_notPerformanceRecognizer(Exception):\n",
        "      \"\"\"Exception raised for not classifier type found\"\"\"\n",
        "\n",
        "      def __init__(self, name):\n",
        "          self.name = name\n",
        "\n",
        "      def __str__(self):\n",
        "          return f\" Clustering not support '{type(self.name)}' performance measure.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Bk1_vo-2YMs"
      },
      "source": [
        "###  Embedding Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "HEBg5Qv-2hCU"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "\n",
        "class VisualEmbedding():\n",
        "\n",
        "    def __init__(self, data, labels, reduction=[\"pca\",\"t-sne\"]):\n",
        "        self.data = data\n",
        "        self.reduction_methods = reduction\n",
        "        self.labels = labels\n",
        "\n",
        "    def embedding_visualization(self,path):\n",
        "        feat_cols = [ 'att_'+str(i) for i in range(len(self.data[0])) ]\n",
        "        df_data = pd.DataFrame(self.data,columns=feat_cols)\n",
        "        df_data['y'] = self.labels\n",
        "        df_data['label'] = df_data['y'].apply(lambda i: str(i))\n",
        "\n",
        "        df_visual = df_data.copy()\n",
        "\n",
        "        #pca-2d\n",
        "        pca = PCA(n_components=3)\n",
        "        pca_result = pca.fit_transform(df_data[feat_cols].values)\n",
        "        df_visual['pca-one'] = pca_result[:,0]\n",
        "        df_visual['pca-two'] = pca_result[:,1]\n",
        "        df_visual['pca-three'] = pca_result[:,2]\n",
        "        print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
        "\n",
        "        #t-sne-2pca\n",
        "        time_start = time.time()\n",
        "        tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
        "        tsne_results = tsne.fit_transform(df_data)\n",
        "        df_visual['tsne-2d-one'] = tsne_results[:,0]\n",
        "        df_visual['tsne-2d-two'] = tsne_results[:,1]\n",
        "        print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
        "\n",
        "\n",
        "        #t-sne-50pca\n",
        "        pca_50 = PCA(n_components=50)\n",
        "        pca_result_50 = pca_50.fit_transform(df_data)\n",
        "        tsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=300)\n",
        "        tsne_pca_results = tsne.fit_transform(pca_result_50)\n",
        "        df_visual['tsne-pca50-one'] = tsne_pca_results[:,0]\n",
        "        df_visual['tsne-pca50-two'] = tsne_pca_results[:,1]\n",
        "        print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
        "\n",
        "        #plot\n",
        "        plt.figure(figsize=(16,7))\n",
        "        ax1 = plt.subplot(2, 2, 1)\n",
        "        sns.scatterplot(\n",
        "            x=\"pca-one\", y=\"pca-two\",\n",
        "            hue=\"y\",\n",
        "            palette=sns.color_palette(\"hls\", 10),\n",
        "            data=df_visual,\n",
        "            legend=\"full\",\n",
        "            alpha=1\n",
        "        )\n",
        "\n",
        "        ax2 = plt.subplot(2, 2, 2)\n",
        "        ax = plt.figure(figsize=(16,10)).gca(projection='3d')\n",
        "        ax.scatter(\n",
        "            xs=df_visual[\"pca-one\"],\n",
        "            ys=df_visual[\"pca-two\"],\n",
        "            zs=df_visual[\"pca-three\"],\n",
        "            c=df_visual[\"y\"],\n",
        "            cmap='tab10'\n",
        "        )\n",
        "        ax.set_xlabel('pca-one')\n",
        "        ax.set_ylabel('pca-two')\n",
        "        ax.set_zlabel('pca-three')\n",
        "\n",
        "        ax3 = plt.subplot(2, 2, 3)\n",
        "        plt.figure(figsize=(16,10))\n",
        "        sns.scatterplot(\n",
        "            x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
        "            hue=\"y\",\n",
        "            palette=sns.color_palette(\"hls\", 10),\n",
        "            data=df_visual,\n",
        "            legend=\"full\",\n",
        "            alpha=1\n",
        "        )\n",
        "\n",
        "        ax4 = plt.subplot(2, 2, 4)\n",
        "        plt.figure(figsize=(16,10))\n",
        "        sns.scatterplot(\n",
        "            x=\"tsne-pca50-one\", y=\"tsne-pca50-two\",\n",
        "            hue=\"y\",\n",
        "            palette=sns.color_palette(\"hls\", 10),\n",
        "            data=df_visual,\n",
        "            legend=\"full\",\n",
        "            alpha=1\n",
        "        )\n",
        "        plt.show()\n",
        "        return df_data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z938dlrSj_CQ"
      },
      "source": [
        "### Testing performance embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "SBnFTj4WkAWI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class PerformanceEmbedding():\n",
        "\n",
        "    def __init__(self, model, embedding_name='att', node_label='node_label'):\n",
        "        if not isinstance(model, GraphEModel):\n",
        "            raise PerformanceEmbedding_notModelClass(model)\n",
        "        self.embedding = model.get_embedding(phase=embedding_name, type_output=\"numpy\")\n",
        "        self.labels = list(model.get_embedding(phase=node_label, type_output=\"numpy\"))\n",
        "        self.group_by = ['avg','sum']\n",
        "        self.cluster_measure = ['rand_score']\n",
        "        self.classifier_measure = ['accuracy_score','precision_macro','precision_micro','precision_weighted',\n",
        "                                   'recall_macro','recall_micro','recall_weighted',\n",
        "                                   'f1_macro','f1_micro','f1_weighted']\n",
        "\n",
        "    def visualization(self):\n",
        "        visualemb = VisualEmbedding(self.embedding,self.labels)\n",
        "        return visualemb.embedding_visualization(None)\n",
        "\n",
        "    def classification(self, repetitions = 10, classifier_name = \"svm\", performance_group_by='avg',labeled_data_threshold=None, measures_selected = None, random_set = True):\n",
        "        if measures_selected is None:\n",
        "            measures_selected = self.classifier_measure\n",
        "        else:\n",
        "            for meas in measures_selected:\n",
        "                if meas not in self.classifier_measure:\n",
        "                    raise PerformanceEmbedding_notMeasureExperiment(meas,'NodeClassification')\n",
        "        if labeled_data_threshold is None:\n",
        "            labeled_data_threshold = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "\n",
        "\n",
        "        n_classify = NodeClassification(self.embedding,self.labels, normalize=True)\n",
        "        measures = dict()\n",
        "        for split_threshold in labeled_data_threshold:\n",
        "            measure = n_classify.classification(classifier_name, split_threshold, repetitions, self.group_by, random_set)\n",
        "            _key = \"split_\"+str(split_threshold)\n",
        "            measures[_key] = measure\n",
        "\n",
        "        return self.performance_measure(measures,measures_selected,performance_group_by)\n",
        "\n",
        "    def clusterization(self, repetitions = 10, performance_group_by='avg', measures_selected = None):\n",
        "        if measures_selected is None:\n",
        "            measures_selected = self.cluster_measure\n",
        "        else:\n",
        "            for meas in measures_selected:\n",
        "                if meas not in self.cluster_measure:\n",
        "                    raise PerformanceEmbedding_notMeasureExperiment(meas,'NodeClustering')\n",
        "\n",
        "        n_clusterfy = NodeClustering(self.embedding,self.labels)\n",
        "        measures = dict()\n",
        "        measure = n_clusterfy.classic_clusterizzation(repetitions, self.group_by)\n",
        "        measures[\"all\"] = measure\n",
        "        return self.performance_measure(measures,measures_selected,performance_group_by)\n",
        "\n",
        "\n",
        "    def performance_measure(self, measures, measures_selected, groub_by='avg',):\n",
        "        pd_measure = pd.DataFrame()\n",
        "        pd_measure['name_measure'] = measures_selected\n",
        "\n",
        "        for split_name in measures:\n",
        "            val_col = list()\n",
        "            for meas_name in measures_selected:\n",
        "                if meas_name not in measures[split_name][groub_by]:\n",
        "                    raise PerformanceEmbedding_notMeasure(meas_name)\n",
        "                else:\n",
        "                    val_col.append(measures[split_name][groub_by][meas_name])\n",
        "            pd_measure[split_name] = val_col\n",
        "        pd_measure.set_index('name_measure')\n",
        "        return pd_measure\n",
        "\n",
        "\n",
        "    def loss_plot(self):\n",
        "        data_plot_losses = [val.item() for val in DAGE_values['losses']]\n",
        "        plt.xlabel('Iterations')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.plot(data_plot_losses,\"b.\")\n",
        "\n",
        "\n",
        "\n",
        "class PerformanceEmbedding_notModelClass(Exception):\n",
        "      \"\"\"Exception raised for not classifier type found\"\"\"\n",
        "\n",
        "      def __init__(self, obj):\n",
        "          self.obj = obj\n",
        "\n",
        "      def __str__(self):\n",
        "          return f\"Model should be a 'GraphEModel' class object but receive a ''{type(self.obj)} object.\"\n",
        "\n",
        "class PerformanceEmbedding_notMeasure(Exception):\n",
        "      \"\"\"Exception raised for not classifier type found\"\"\"\n",
        "\n",
        "      def __init__(self, name_measure):\n",
        "          self.name_measure = name_measure\n",
        "\n",
        "      def __str__(self):\n",
        "          return f\"Percormance '{self.name_measure}' not recognized.\"\n",
        "\n",
        "class PerformanceEmbedding_notMeasureExperiment(Exception):\n",
        "      \"\"\"Exception raised for not classifier type found\"\"\"\n",
        "\n",
        "      def __init__(self, measure_name, experiment_name):\n",
        "          self.measure_name = measure_name\n",
        "          self.experiment_name = experiment_name\n",
        "\n",
        "      def __str__(self):\n",
        "          return f\"Experiment '{self.experiment_name}' not implement performance called '{self.measure_name}'.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQvNRHHXIlSa"
      },
      "source": [
        " ## DEMO on the CORA Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "THtrE_kb-e5R"
      },
      "outputs": [],
      "source": [
        "torch.set_printoptions(edgeitems=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MDBo5izsd58",
        "outputId": "560377ef-9e6b-4bda-a289-5715f9e4a61a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'CAGE' already exists and is not an empty directory.\n",
            "'cp' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "datase_name = 'cora'\n",
        "path_models_checkpoint = '/content/models_checkpoint'\n",
        "Util_class.folder_manage(path_models_checkpoint)\n",
        "path = '/content/dataset/'\n",
        "Util_class.folder_manage(path)\n",
        "!git clone https://github.com/MIND-Lab/CAGE.git\n",
        "!cp -r /content/CAGE/data/* /content/dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-iM6-7FxV0c"
      },
      "source": [
        "### CAGE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8i1i5ZbxV04"
      },
      "source": [
        "#### SETTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sbm8Cu_xV06",
        "outputId": "48a04c4d-70f7-460b-e0d6-c5fe9f5ce0bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Structure dimension:\t 2708\n",
            "Attribute dimension:\t 1433\n",
            "cora\n"
          ]
        }
      ],
      "source": [
        "edge_file_name = f\"CAGE/data/{datase_name}/in_edges.txt\"\n",
        "attribute_file_name = f\"CAGE/data/{datase_name}/in_features.txt\"\n",
        "label_file_name = f\"CAGE/data/{datase_name}/in_group.txt\"\n",
        "\n",
        "path_model_checkpoint = f\"/content/models_checkpoint/CAGE_{datase_name}\"\n",
        "Util_class.folder_manage(path_model_checkpoint)\n",
        "\n",
        "is_directed_graph = False\n",
        "attribute_file_format = \"normalized_matrix\"\n",
        "dataLoad = LoadDataset(edge_file_name,attribute_file_name,label_file_name,attribute_file_format=attribute_file_format,is_directed_graph=is_directed_graph)\n",
        "\n",
        "dataLoad.export_graph(f\"/content/models_checkpoint/CAGE_{datase_name}\",f\"CAGE_{datase_name}_graph\")\n",
        "net_adj = dataLoad.get_structural_matrix()\n",
        "att_adj = dataLoad.get_attribute_matrix()\n",
        "label_vec = dataLoad.get_labels()\n",
        "print(datase_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "39JKpW-fxV07"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "shuffle = True\n",
        "net_hadmard_coeff = 5.0\n",
        "att_hadmard_coeff = 5.0\n",
        "\n",
        "batchGenerator = DataBatchGenerator(net_adj, att_adj, label_vec, batch_size, shuffle, net_hadmard_coeff, att_hadmard_coeff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Knn1g86hguGy"
      },
      "source": [
        "#### TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "C8qFWSngxV07"
      },
      "outputs": [],
      "source": [
        "CAGE_net_layers_list = [\n",
        "    {'type':'DENSE','features':128,'act_funtion':'RELU','bias':True}\n",
        "]\n",
        "\n",
        "CAGE_att_layers_list = [\n",
        "    {'type':'KCOMP','ktop':200,'alpha_factor': 3.0},\n",
        "    {'type':'DENSE','features':128,'act_funtion':'RELU','bias':True}\n",
        "]\n",
        "\n",
        "CAGE_loss_settings_list = {\n",
        "    'all' : [\n",
        "    ],\n",
        "    'net':[\n",
        "           {'loss_name':\"structur_proximity_1order\",'coef':1},\n",
        "           {'loss_name':\"structur_proximity_2order\",'coef':1},\n",
        "    ],\n",
        "    'att':[\n",
        "           {'loss_name':\"semantic_proximity_2order\",'coef':1},\n",
        "           {'loss_name':\"square_diff_embedding_proximity\",'coef':1},\n",
        "    ],\n",
        "}\n",
        "\n",
        "CAGE_optimizator_net_settings_list = {\n",
        "        \"opt_name\" : \"adam\",\n",
        "        \"lr_rate\" : 1e-4\n",
        "}\n",
        "\n",
        "CAGE_optimizator_att_settings_list = {\n",
        "        \"opt_name\" : \"adam\",\n",
        "        \"lr_rate\" : 1e-3\n",
        "}\n",
        "\n",
        "CAGE_regularization_net_settings_list = [\n",
        "    {'reg_name': 'L2', 'coeff': 0.001}\n",
        "]\n",
        "\n",
        "CAGE_regularization_att_settings_list = [\n",
        "    {'reg_name': 'L2', 'coeff': 0.001}\n",
        "]\n",
        "\n",
        "CAGE_checkpoint_config ={\n",
        "    \"type\" : [\"best_model_loss\",\"first_train\",\"last_train\"],\n",
        "    \"times\": 20,\n",
        "    \"overwrite\" : False,\n",
        "    \"path_file\" : f\"/content/models_checkpoint/CAGE_{datase_name}\",\n",
        "    \"name_file\" : f\"CAGE_{datase_name}_checkpoint\",\n",
        "    \"path_not_exist\": \"create\",\n",
        "    \"path_exist\": \"use\",\n",
        "}\n",
        "\n",
        "\n",
        "CAGE_config = {\n",
        "    \"net_dim\" : dataLoad.get_input_shape('net'),\n",
        "    \"net_layers_list\" : CAGE_net_layers_list,\n",
        "    \"net_latent_dim\" : 128,\n",
        "\n",
        "\n",
        "    \"att_dim\" : dataLoad.get_input_shape('att'),\n",
        "    \"att_layers_list\" : CAGE_att_layers_list,\n",
        "    \"att_latent_dim\" : 128,\n",
        "\n",
        "   \"loss_functions\" : CAGE_loss_settings_list,\n",
        "\n",
        "   \"optimizator_net\" : CAGE_optimizator_net_settings_list,\n",
        "   \"optimizator_att\" : CAGE_optimizator_att_settings_list,\n",
        "\n",
        "   \"regularization_net\" : CAGE_regularization_net_settings_list,\n",
        "   \"regularization_att\" : CAGE_regularization_att_settings_list,\n",
        "\n",
        "   \"checkpoint_config\" : CAGE_checkpoint_config,\n",
        "   \"model_name\" : \"CAGE_wiki_opt1\",\n",
        "   \"training_config\" : \"N>A\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKUnvrUYxV09",
        "outputId": "9136491a-f8d9-469e-afff-a91b53c48bf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your model's checkpoint is save in : /content/models_checkpoint/CAGE_cora\n",
            "=\n",
            "Phase :  net \tEpoch :  1 / 10 \tLoss :  8983.334708280341 \tlr net:  0.0001 \tlr att:  0.001\n",
            "Epoch :  1 / 10 \tLoss :  8983.334708280341 \tmodel checkpoint saved as: /content/models_checkpoint/CAGE_cora/CAGE_cora_checkpoint_phasenet_epoch_1.carbo\n",
            "==\n",
            "Phase :  net \tEpoch :  2 / 10 \tLoss :  8908.606309047966 \tlr net:  0.0001 \tlr att:  0.001\n",
            "=\n",
            "Phase :  net \tEpoch :  3 / 10 \tLoss :  8750.491199582122 \tlr net:  0.0001 \tlr att:  0.001\n",
            "==\n",
            "Phase :  net \tEpoch :  4 / 10 \tLoss :  8447.238644622093 \tlr net:  0.0001 \tlr att:  0.001\n",
            "=\n",
            "Phase :  net \tEpoch :  5 / 10 \tLoss :  8001.613715593205 \tlr net:  0.0001 \tlr att:  0.001\n",
            "==\n",
            "Phase :  net \tEpoch :  6 / 10 \tLoss :  7443.724010378815 \tlr net:  0.0001 \tlr att:  0.001\n",
            "=\n",
            "Phase :  net \tEpoch :  7 / 10 \tLoss :  6833.792168195858 \tlr net:  0.0001 \tlr att:  0.001\n",
            "==\n",
            "Phase :  net \tEpoch :  8 / 10 \tLoss :  6229.178480991098 \tlr net:  0.0001 \tlr att:  0.001\n",
            "=\n",
            "Phase :  net \tEpoch :  9 / 10 \tLoss :  5656.772925088572 \tlr net:  0.0001 \tlr att:  0.001\n",
            "==\n",
            "Phase :  net \tEpoch :  10 / 10 \tLoss :  5137.185206213663 \tlr net:  0.0001 \tlr att:  0.001\n",
            "Epoch :  10 / 10 \tLoss :  5137.185206213663 \tmodel checkpoint saved as: /content/models_checkpoint/CAGE_cora/CAGE_cora_checkpoint_phasenet_epoch_10.carbo\n",
            "Set embedding for:\t net\n",
            "Saved embedding for:\t net\t\t on path:\t/content/models_checkpoint/CAGE_cora/embedding_CAGE_wiki_opt1_net.ecarbo\n",
            "=\n",
            "Phase :  att \tEpoch :  1 / 10 \tLoss :  575.9678905398347 \tlr net:  0.0001 \tlr att:  0.001\n",
            "Epoch :  1 / 10 \tLoss :  575.9678905398347 \tmodel checkpoint saved as: /content/models_checkpoint/CAGE_cora/CAGE_cora_checkpoint_phaseatt_epoch_1.carbo\n",
            "==\n",
            "Phase :  att \tEpoch :  2 / 10 \tLoss :  505.9041442871094 \tlr net:  0.0001 \tlr att:  0.001\n",
            "=\n",
            "Phase :  att \tEpoch :  3 / 10 \tLoss :  440.68006471145986 \tlr net:  0.0001 \tlr att:  0.001\n",
            "==\n",
            "Phase :  att \tEpoch :  4 / 10 \tLoss :  386.09512648471565 \tlr net:  0.0001 \tlr att:  0.001\n",
            "=\n",
            "Phase :  att \tEpoch :  5 / 10 \tLoss :  332.95067170608877 \tlr net:  0.0001 \tlr att:  0.001\n",
            "==\n",
            "Phase :  att \tEpoch :  6 / 10 \tLoss :  272.308059514955 \tlr net:  0.0001 \tlr att:  0.001\n",
            "=\n",
            "Phase :  att \tEpoch :  7 / 10 \tLoss :  222.15354129879972 \tlr net:  0.0001 \tlr att:  0.001\n",
            "==\n",
            "Phase :  att \tEpoch :  8 / 10 \tLoss :  182.46447558735693 \tlr net:  0.0001 \tlr att:  0.001\n",
            "=\n",
            "Phase :  att \tEpoch :  9 / 10 \tLoss :  154.3815846997638 \tlr net:  0.0001 \tlr att:  0.001\n",
            "==\n",
            "Phase :  att \tEpoch :  10 / 10 \tLoss :  130.96299752523734 \tlr net:  0.0001 \tlr att:  0.001\n",
            "Epoch :  10 / 10 \tLoss :  130.96299752523734 \tmodel checkpoint saved as: /content/models_checkpoint/CAGE_cora/CAGE_cora_checkpoint_phaseatt_epoch_10.carbo\n",
            "Set embedding for:\t att\n",
            "Saved embedding for:\t att\t\t on path:\t/content/models_checkpoint/CAGE_cora/embedding_CAGE_wiki_opt1_att.ecarbo\n"
          ]
        }
      ],
      "source": [
        "CAGE_model = GraphEModel(CAGE_config)\n",
        "CAGE_epochs_config = {\n",
        "    'att' : 10,\n",
        "    'net' : 10,\n",
        "}\n",
        "\n",
        "DAGE_values = CAGE_model.models_training(datagenerator=batchGenerator, loss_verbose=False,epochs=CAGE_epochs_config, path_embedding=f\"/content/models_checkpoint/CAGE_{datase_name}/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWkykEbElfyj"
      },
      "source": [
        "#### TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "NRmBTMgPDPBl",
        "outputId": "45bcd897-314b-4a9d-c902-5b45848f37d3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name_measure</th>\n",
              "      <th>split_0.1</th>\n",
              "      <th>split_0.2</th>\n",
              "      <th>split_0.3</th>\n",
              "      <th>split_0.4</th>\n",
              "      <th>split_0.5</th>\n",
              "      <th>split_0.6</th>\n",
              "      <th>split_0.7</th>\n",
              "      <th>split_0.8</th>\n",
              "      <th>split_0.9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>accuracy_score</td>\n",
              "      <td>0.721402</td>\n",
              "      <td>0.724354</td>\n",
              "      <td>0.724846</td>\n",
              "      <td>0.723339</td>\n",
              "      <td>0.714623</td>\n",
              "      <td>0.708308</td>\n",
              "      <td>0.698840</td>\n",
              "      <td>0.681726</td>\n",
              "      <td>0.633880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>precision_macro</td>\n",
              "      <td>0.724324</td>\n",
              "      <td>0.719639</td>\n",
              "      <td>0.718935</td>\n",
              "      <td>0.719428</td>\n",
              "      <td>0.703903</td>\n",
              "      <td>0.704677</td>\n",
              "      <td>0.698466</td>\n",
              "      <td>0.687678</td>\n",
              "      <td>0.651061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>precision_micro</td>\n",
              "      <td>0.721402</td>\n",
              "      <td>0.724354</td>\n",
              "      <td>0.724846</td>\n",
              "      <td>0.723339</td>\n",
              "      <td>0.714623</td>\n",
              "      <td>0.708308</td>\n",
              "      <td>0.698840</td>\n",
              "      <td>0.681726</td>\n",
              "      <td>0.633880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>precision_weighted</td>\n",
              "      <td>0.724554</td>\n",
              "      <td>0.725328</td>\n",
              "      <td>0.725280</td>\n",
              "      <td>0.723918</td>\n",
              "      <td>0.713598</td>\n",
              "      <td>0.708798</td>\n",
              "      <td>0.701407</td>\n",
              "      <td>0.684808</td>\n",
              "      <td>0.641854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>recall_macro</td>\n",
              "      <td>0.687765</td>\n",
              "      <td>0.689540</td>\n",
              "      <td>0.691238</td>\n",
              "      <td>0.683233</td>\n",
              "      <td>0.677938</td>\n",
              "      <td>0.667241</td>\n",
              "      <td>0.656423</td>\n",
              "      <td>0.630675</td>\n",
              "      <td>0.567270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>recall_micro</td>\n",
              "      <td>0.721402</td>\n",
              "      <td>0.724354</td>\n",
              "      <td>0.724846</td>\n",
              "      <td>0.723339</td>\n",
              "      <td>0.714623</td>\n",
              "      <td>0.708308</td>\n",
              "      <td>0.698840</td>\n",
              "      <td>0.681726</td>\n",
              "      <td>0.633880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>recall_weighted</td>\n",
              "      <td>0.721402</td>\n",
              "      <td>0.724354</td>\n",
              "      <td>0.724846</td>\n",
              "      <td>0.723339</td>\n",
              "      <td>0.714623</td>\n",
              "      <td>0.708308</td>\n",
              "      <td>0.698840</td>\n",
              "      <td>0.681726</td>\n",
              "      <td>0.633880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.699853</td>\n",
              "      <td>0.700612</td>\n",
              "      <td>0.701253</td>\n",
              "      <td>0.696633</td>\n",
              "      <td>0.687861</td>\n",
              "      <td>0.681736</td>\n",
              "      <td>0.671458</td>\n",
              "      <td>0.648996</td>\n",
              "      <td>0.588295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>f1_micro</td>\n",
              "      <td>0.721402</td>\n",
              "      <td>0.724354</td>\n",
              "      <td>0.724846</td>\n",
              "      <td>0.723339</td>\n",
              "      <td>0.714623</td>\n",
              "      <td>0.708308</td>\n",
              "      <td>0.698840</td>\n",
              "      <td>0.681726</td>\n",
              "      <td>0.633880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>f1_weighted</td>\n",
              "      <td>0.717588</td>\n",
              "      <td>0.721745</td>\n",
              "      <td>0.721747</td>\n",
              "      <td>0.720076</td>\n",
              "      <td>0.711854</td>\n",
              "      <td>0.704964</td>\n",
              "      <td>0.695700</td>\n",
              "      <td>0.676483</td>\n",
              "      <td>0.622907</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         name_measure  split_0.1  split_0.2  split_0.3  split_0.4  split_0.5  \\\n",
              "0      accuracy_score   0.721402   0.724354   0.724846   0.723339   0.714623   \n",
              "1     precision_macro   0.724324   0.719639   0.718935   0.719428   0.703903   \n",
              "2     precision_micro   0.721402   0.724354   0.724846   0.723339   0.714623   \n",
              "3  precision_weighted   0.724554   0.725328   0.725280   0.723918   0.713598   \n",
              "4        recall_macro   0.687765   0.689540   0.691238   0.683233   0.677938   \n",
              "5        recall_micro   0.721402   0.724354   0.724846   0.723339   0.714623   \n",
              "6     recall_weighted   0.721402   0.724354   0.724846   0.723339   0.714623   \n",
              "7            f1_macro   0.699853   0.700612   0.701253   0.696633   0.687861   \n",
              "8            f1_micro   0.721402   0.724354   0.724846   0.723339   0.714623   \n",
              "9         f1_weighted   0.717588   0.721745   0.721747   0.720076   0.711854   \n",
              "\n",
              "   split_0.6  split_0.7  split_0.8  split_0.9  \n",
              "0   0.708308   0.698840   0.681726   0.633880  \n",
              "1   0.704677   0.698466   0.687678   0.651061  \n",
              "2   0.708308   0.698840   0.681726   0.633880  \n",
              "3   0.708798   0.701407   0.684808   0.641854  \n",
              "4   0.667241   0.656423   0.630675   0.567270  \n",
              "5   0.708308   0.698840   0.681726   0.633880  \n",
              "6   0.708308   0.698840   0.681726   0.633880  \n",
              "7   0.681736   0.671458   0.648996   0.588295  \n",
              "8   0.708308   0.698840   0.681726   0.633880  \n",
              "9   0.704964   0.695700   0.676483   0.622907  "
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CAGE_performances = PerformanceEmbedding(CAGE_model,embedding_name='att')\n",
        "\n",
        "CAGE_class = CAGE_performances.classification(repetitions=10)\n",
        "CAGE_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "19R9Ghc4elqt",
        "outputId": "359ab42d-0578-4376-ece2-701ad95a8c7b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name_measure</th>\n",
              "      <th>all</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rand_score</td>\n",
              "      <td>0.79252</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  name_measure      all\n",
              "0   rand_score  0.79252"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CAGE_clust = CAGE_performances.clusterization(repetitions=10)\n",
        "CAGE_clust"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "# assume your DataFrame is called pd_measure\n",
        "CAGE_class.to_csv(\"cage_performance.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "M7CBO8ZSIhsk",
        "8F7n67NXnETx",
        "RZvfKlejSPFu",
        "0NCBL96pIIcM",
        "lbfAuKRKF1Nr",
        "fqki-zA6Y7AF",
        "otlEnyajIuru",
        "_zAeQRL6I1dI",
        "KppIu4dSEp9f",
        "Z_u48wicRIzb",
        "jdKXGPdtYGog",
        "MjmJCzGZJI-u",
        "5mSXrquB_0bZ",
        "UrN99AyKeXbW",
        "9Bk1_vo-2YMs",
        "Z938dlrSj_CQ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
